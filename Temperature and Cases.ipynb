{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data from: Heyes, Anthony, and Soodeh Saberian. 2019. \"Temperature and Decisions: Evidence from 207,000 Court Cases.\" American Economic Journal: Applied Economics, 11 (2): 238â€“65.\n",
    "\n",
    "Notebooks used troughout the code: \n",
    "- ISLP-Ch06_varselect_lab.ipynb\n",
    "- ISLP-TreeModels.ipynb\n",
    "- CIDP-Chapter_04\n",
    "- CIDP-Chapter_05\n",
    "- CIDP-Chapter_07\n",
    "\n",
    "Code for Project 2 related activities:\n",
    "- CIBT-11-Propensity-Score\n",
    "- CIBT-21 Meta-Learners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import sklearn.linear_model as skl\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sklearn.model_selection as skm\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from matplotlib.pyplot import subplots\n",
    "from statsmodels.discrete.discrete_model import Probit\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "!pip install stargazer\n",
    "from stargazer.stargazer import Stargazer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Tree\n",
    "from sklearn.tree import (DecisionTreeClassifier as DTC,\n",
    "                          DecisionTreeRegressor as DTR,\n",
    "                          plot_tree,\n",
    "                          export_text)\n",
    "from sklearn.metrics import (accuracy_score,\n",
    "                             log_loss)\n",
    "from sklearn.ensemble import \\\n",
    "     (RandomForestClassifier as RFC,\n",
    "      GradientBoostingClassifier as GBC)\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Graphs\n",
    "from scipy import stats\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "import graphviz\n",
    "import networkx as nx\n",
    "COLORS = [\n",
    "    '#00B0F0',\n",
    "    '#FF0000'\n",
    "]\n",
    "import matplotlib.patches as patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DAG\n",
    "!pip install dowhy\n",
    "import dowhy\n",
    "from dowhy import CausalModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Propensity Score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Description "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the dataset from the replication package\n",
    "df = pd.read_stata('matched_corrected.dta')\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the list of columns to identify all variables\n",
    "\n",
    "columns_list = df.columns.tolist()\n",
    "print(columns_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dummy for asylum\n",
    "df['dummy_asylum'] = df['c_asy_type'].apply(lambda x: 1 if x == 'E' else 0)\n",
    "#Create a dummy for gender\n",
    "df['dummy_gender'] = df['gender'].apply(lambda x: 1 if x == 'female' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As outlined in the correction article drop the observation for China\n",
    "df = df[df['nat_name'] != 'CHINA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique values to identify variables for the dummy variables\n",
    "unique__names = df['nat_name'].unique()\n",
    "locations = df['location'].unique()\n",
    "\n",
    "#Create a categorical variable for nationatility\n",
    "\n",
    "# Define the list of regions\n",
    "middle_eastern_countries = [\"BAHRAIN\", \"CYPRUS\", \"EGYPT\", \"IRAN\", \"IRAQ\", \"ISRAEL\", \"JORDAN\", \n",
    "    \"KUWAIT\", \"LEBANON\", \"OMAN\", \"PALESTINE\", \"QATAR\", \"SAUDI ARABIA\", \n",
    "    \"SYRIA\", \"TURKEY\", \"UNITED ARAB EMIRATES\", \"YEMEN\"]\n",
    "\n",
    "africa = [\"ERITREA\", \"RWANDA\", \"SOMALIA\", \"SUDAN\", \"CONGO\", \"ETHIOPIA\", \"LIBYA\", \n",
    "    \"MALI\", \"ANGOLA\", \"BURUNDI\", \"TANZANIA\", \"NIGERIA\", \"GABON\", \"GHANA\", \n",
    "    \"SENEGAL\", \"CHAD\", \"DJIBOUTI\", \"CAMEROON\", \"UGANDA\", \"KENYA\", \n",
    "    \"ZAMBIA\", \"MAURITANIA\", \"SOUTH AFRICA\", \"GUINEA\", \"BURKINA FASO\", \n",
    "    \"MOROCCO\", \"ALGERIA\", \"COMORO ISLANDS\", \"EQUATORIAL GUINEA\", \n",
    "    \"CENTRAL AFRICAN REPUBLIC\", \"CAPE VERDE\", \"LESOTHO\", \"SWAZILAND\", \n",
    "    \"GAMBIA\", \"SIERRA LEONE\", \"GUINEA BISSAU\"]\n",
    "\n",
    "america = [\"GUATEMALA\", \"EL SALVADOR\", \"PANAMA\", \"COLOMBIA\", \n",
    "    \"ARGENTINA\", \"HAITI\", \"VENEZUELA\", \"MEXICO\", \"CUBA\", \"DOMINICAN REPUBLIC\", \n",
    "    \"BRAZIL\", \"CHILE\", \"SURINAME\", \"TRINIDAD AND TOBAGO\", \"JAMAICA\", \n",
    "    \"CANADA\", \"USA\", \"ST. KITTS, WEST INDIES\", \"ANTIGUA AND BARBUDA\", \n",
    "    \"BARBADOS\", \"BAHAMAS\", \"BELIZE\", \"DOMINICA\", \"GRENADA\", \n",
    "    \"NICARAGUA\", \"URUGUAY\", \"PARAGUAY\", \"ST. LUCIA\", \"ST. VINCENT AND THE GRENADINES\"]\n",
    "\n",
    "asia = [\"PAKISTAN\", \"VIETNAM\", \"INDONESIA\", \"AFGHANISTAN\", \n",
    "    \"IRAN\", \"BANGLADESH\", \"PHILIPPINES\", \"TAIWAN\", \"MALAYSIA\", \n",
    "    \"KAZAKHSTAN\", \"KYRGYZSTAN\", \"THAILAND\", \"TURKMENISTAN\", \"UZBEKISTAN\", \n",
    "    \"MONGOLIA\", \"SRI LANKA\", \"BHUTAN\", \"LAOS\", \"NEPAL\", \n",
    "    \"MYANMAR\", \"KAMPUCHEA\", \"BRUNEI\", \"BURMA\", \"KOREA\", \"NORTH KOREA\"]\n",
    "\n",
    "europe = [\"RUSSIA\", \"ARMENIA\", \"ALBANIA\", \"YUGOSLAVIA\", \"UNITED KINGDOM\", \n",
    "    \"BULGARIA\", \"ROMANIA\", \"HUNGARY\", \"POLAND\", \"CZECH REPUBLIC\", \n",
    "    \"SLOVAK REPUBLIC\", \"GERMANY\", \"FRANCE\", \"ITALY\", \"SPAIN\", \n",
    "    \"SWEDEN\", \"DENMARK\", \"FINLAND\", \"AUSTRIA\", \"SWITZERLAND\", \n",
    "    \"BELGIUM\", \"GREECE\", \"NETHERLANDS\", \"CROATIA\", \"SLOVENIA\", \n",
    "    \"MONACO\", \"LITHUANIA\", \"LATVIA\", \"ESTONIA\", \"ICELAND\"]\n",
    "\n",
    "# Create the regional variable and set it to 0 by default\n",
    "df['middleast'] = 0\n",
    "df['america'] = 0\n",
    "df['africa'] = 0\n",
    "df['asia'] = 0\n",
    "df['europe'] = 0\n",
    "\n",
    "# Replace with 1 for observations where nationality is in the list of selected regions\n",
    "df.loc[df['nat_name'].isin(middle_eastern_countries), 'middleast'] = 1\n",
    "df.loc[df['nat_name'].isin(america), 'america'] = 1\n",
    "df.loc[df['nat_name'].isin(africa), 'africa'] = 1\n",
    "df.loc[df['nat_name'].isin(asia), 'asia'] = 1\n",
    "df.loc[df['nat_name'].isin(europe), 'europe'] = 1\n",
    "\n",
    "#Create interaction terms\n",
    "df['middleast_dev'] = df['middleast']*df['temp6t4']\n",
    "df['america_dev'] = df['america']*df['temp6t4']\n",
    "df['africa_dev'] = df['africa']*df['temp6t4']\n",
    "df['asia_dev'] = df['asia']*df['temp6t4']\n",
    "df['europe_dev'] = df['europe']*df['temp6t4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a categorical variable for location and group locations into regions\n",
    "northeast = ['NEWARK', 'BOSTON', 'NEW YORK CITY', 'BUFFALO', 'PHILADELPHIA', \n",
    "    'NEW YORK ANNEX', 'NY DET (VARICK ST.)', 'HARTFORD', \n",
    "    '*PA DOC.', 'CLEVELAND', '*BOP  DANBURY', '*RI  DOC',\n",
    "    '*WISCONSIN DOC', '*NH  DOC', '*SUFFOLK COUNTY','*NEWARK VIDEO HEARINGS','*JESSUP'\n",
    "    '*BOP ALLENWOOD', '*NORTHERN STATE NJ DOC','YORK COUNTY DET','YORK COUNTY DET']\n",
    "\n",
    "midwest = ['CHICAGO', 'DETROIT', 'CINCINNATI', 'CLEVELAND', 'ST. LOUIS', \n",
    "    'MEMPHIS', 'KANSAS CITY', 'OMAHA', '*MI  DOC', \n",
    "    '*IL DOC - STATESVILLE', '*MO DOC', '*OHIO DOC', \n",
    "    '*INDIANA YOUTH CENTER']\n",
    "\n",
    "south = ['ARLINGTON', 'DALLAS', 'HOUSTON', 'MIAMI', 'ATLANTA', \n",
    "    'NEW ORLEANS', 'SAN ANTONIO', 'DALLAS DET', 'SAN ANTONIO DET', \n",
    "    'HOUSTON DET', 'ATLANTA DET', '*GEORGIA DOC', '*VA DOC', \n",
    "    '*DADE COUNTY FL DOC', '*BROWARD  FL DOC', 'ORLANDO', 'KROME DET',\n",
    "    'PORT ISABEL DET', 'EL PASO', 'EL PASO DET', '*TX DOC', \n",
    "    'LOUISVILLE', 'OKLAHOMA CITY', 'OKLAHOMA CITY DET', \n",
    "    'BATAVIA SPC', 'BROWARD TRANS CTR','ST. THOMAS', 'ST. CROIX', 'ROLLING PLAINS DETENTION CENTER',\n",
    "    '*BOP BIG SPRING AIRPARK','BRADENTON DET','SAN ANTONIO DET']\n",
    "\n",
    "west = ['DENVER', 'SAN DIEGO', 'LOS ANGELES', 'SAN FRANCISCO', \n",
    "    'PHOENIX', 'LAS VEGAS', 'RENO', 'SALT LAKE CITY', 'OTAY MESA', \n",
    "    'TUCSON', 'HONOLULU', 'SAN JUAN', 'SEATTLE', 'PORTLAND',\n",
    "    'SAN FRANCISCO DET', 'DENVER DET', 'SAN DIEGO DETAINED', \n",
    "    'MIRA LOMA DET', 'HONOLULU DET', '*CO DOC', '*AZ DOC',\n",
    "    '*WA DOC', '*AK DOC', 'ANCHORAGE', 'SAN PEDRO', \n",
    "    'IMPERIAL', '*NM DOC','PORTLAND DET','*MONROE WA DOC','SAN FRANCISCO ANNEX']\n",
    "\n",
    "\n",
    "# Create the regional variable and set it to 0 by default\n",
    "df['northeast'] = 0\n",
    "df['midwest'] = 0\n",
    "df['south'] = 0\n",
    "df['west'] = 0\n",
    "\n",
    "\n",
    "# Replace with 1 for observation where location is in the list of selected regions\n",
    "df.loc[df['location'].isin(northeast), 'northeast'] = 1\n",
    "df.loc[df['location'].isin(midwest), 'midwest'] = 1\n",
    "df.loc[df['location'].isin(south), 'south'] = 1\n",
    "df.loc[df['location'].isin(west), 'west'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a date categorical variable\n",
    "df['year'] = df['date'].dt.year\n",
    "\n",
    "#create dummy for year\n",
    "df['year2000'] = 0\n",
    "df['year2001'] = 0\n",
    "df['year2002'] = 0\n",
    "df['year2003'] = 0\n",
    "df['year2004'] = 0\n",
    "\n",
    "# Replace with 1 for observations in a specific year\n",
    "df.loc[df['year'] == 2000, 'year2000'] = 1\n",
    "df.loc[df['year'] == 2001, 'year2001'] = 1\n",
    "df.loc[df['year'] == 2002, 'year2002'] = 1\n",
    "df.loc[df['year'] == 2003, 'year2003'] = 1\n",
    "df.loc[df['year'] == 2004, 'year2004'] = 1\n",
    "\n",
    "\n",
    "# Interaction term for location and year\n",
    "years = [2000, 2001, 2002, 2003, 2004]\n",
    "locations = ['northeast', 'midwest', 'south', 'west']\n",
    "\n",
    "for year in years:\n",
    "    for location in locations:\n",
    "        df[f'{location}_year{year}'] = df[location] * df[f'year{year}']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy variables for the months \n",
    "\n",
    "df['month'] = df['date'].dt.month\n",
    "df = pd.get_dummies(df, columns=['month'], prefix='month', drop_first=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean the dataset\n",
    "\n",
    "#Drop asylum cases with no classification\n",
    "df = df[df['c_asy_type'].isin(['E', 'I'])]\n",
    "\n",
    "# Clean dataset by dropping any rows with NA observations\n",
    "df_final = df.dropna(axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the summary statistics for the main variables of interest\n",
    "summary_stats = df_final[['temp6t4','heat','wind6t4','res','dummy_gender']].describe() \n",
    "print(summary_stats)\n",
    "\n",
    "mean_values = df_final[['temp6t4','heat','wind6t4','res','dummy_gender']].mean()\n",
    "print(mean_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean\n",
    "mean_values = df_final[['temp6t4', 'heat', 'wind6t4']].mean()\n",
    "\n",
    "# Calculate median\n",
    "median_values = df_final[['temp6t4', 'heat', 'wind6t4']].median()\n",
    "\n",
    "# Display results\n",
    "print(\"Mean values:\\n\", mean_values)\n",
    "print(\"\\nMedian values:\\n\", median_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "# Define the intervals for the histogram (customize these as needed)\n",
    "bins = [0, 10, 20, 30, 40, 50, 60, 70, 80,90,100]  # Adjust the bin edges according to your data\n",
    "labels = [f\"{bins[i]}-{bins[i+1]}\" for i in range(len(bins)-1)]  # Create labels for the intervals\n",
    "\n",
    "# Create a new column for the binned data\n",
    "df_final['temp_bins'] = pd.cut(df_final['temp6t4'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "# Count the occurrences in each bin\n",
    "counts = df_final['temp_bins'].value_counts().sort_index()\n",
    "\n",
    "# Plot the histogram\n",
    "plt.figure(figsize=(7, 6), facecolor='white')  \n",
    "ax = plt.subplot(111, facecolor='white')  \n",
    "counts.plot(kind='bar', color='skyblue')\n",
    "plt.xlabel('Temperature (Â°F)')\n",
    "plt.ylabel('Count of Cases')\n",
    "plt.xticks(rotation=45) \n",
    "plt.grid(axis='y')\n",
    "\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_visible(False)\n",
    "    \n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summary statistics per region to check if the observations are balanced\n",
    "\n",
    "#List of locations\n",
    "locations = ['northeast', 'midwest', 'south', 'west']\n",
    "\n",
    "for location in locations:\n",
    "    # Filter rows where the location is 1 \n",
    "    df_filtered = df_final[df_final[location] == 1]\n",
    "    # Calculate summary statistics for resolution in the filtered data\n",
    "    summary_stats = df_filtered['res'].describe()\n",
    "    # Print the summary stats for the current location\n",
    "    print(f\"Summary statistics for {location}:\")\n",
    "    print(summary_stats)\n",
    "    print(\"\\n\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average and standard deviation for the dummy_gender variable\n",
    "mean_value = df_final['dummy_gender'].mean()\n",
    "std_dev = df_final['dummy_gender'].std()\n",
    "\n",
    "# Prepare data for the bar plot\n",
    "summary = pd.DataFrame({\n",
    "    'Gender': ['Male'],  \n",
    "    'Mean': [1-mean_value], \n",
    "    'Std Dev': [std_dev]  \n",
    "})\n",
    "\n",
    "# Create the bar plot\n",
    "plt.figure(figsize=(7, 6))\n",
    "plt.figure(figsize=(7, 6), facecolor='white')  \n",
    "ax = plt.subplot(111, facecolor='white')  \n",
    "bars = plt.bar(summary['Gender'], summary['Mean'], yerr=summary['Std Dev'], \n",
    "                capsize=5, color=['skyblue', 'lightcoral'])\n",
    "\n",
    "# Add labels and title\n",
    "plt.ylabel('Proportion')\n",
    "\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_visible(False)\n",
    "\n",
    "# Show the plot\n",
    "plt.xticks(rotation=0)  \n",
    "plt.tight_layout()       \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create my Y variable\n",
    "Y = np.array(df_final['res'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create X variables for different specifications\n",
    "#Note: drop one category for each dummy\n",
    "\n",
    "\n",
    "#Specification 1\n",
    "selectedvariables = ['temp6t4','heat','skycover', 'co', 'co_distance', 'ozone', 'ozone_distance', 'pm', \n",
    "                     'pm_distance', 'press6t4', 'dew6t4', 'prcp6t4', 'wind6t4', \n",
    "                     'rh6t4', 'chair', 'dummy_asylum', 'dummy_gender', \n",
    "                     'middleast', 'america', 'africa', 'europe', 'northeast', 'midwest', \n",
    "                     'south', 'year2000', 'year2001', 'year2002', \n",
    "                     'year2003','middleast_dev','america_dev','africa_dev','europe_dev','month_1',\n",
    "                     'month_2','month_3','month_4','month_5','month_6','month_7','month_8',\n",
    "                     'month_9','month_10','month_11']\n",
    "\n",
    "#Specification 2\n",
    "selectedvariables_noweather = ['temp6t4','chair', 'dummy_asylum', 'dummy_gender', \n",
    "                     'middleast', 'america', 'africa', 'europe', 'northeast', 'midwest', \n",
    "                     'south', 'year2000', 'year2001', 'year2002', \n",
    "                     'year2003','middleast_dev','america_dev','africa_dev','europe_dev','month_1','month_2',\n",
    "                      'month_3','month_4','month_5','month_6','month_7','month_8',\n",
    "                    'month_9','month_10','month_11']\n",
    "\n",
    "#Additional Fixed Effects\n",
    "year_location = ['northeast_year2000', 'northeast_year2001', 'northeast_year2002', 'northeast_year2003', \n",
    "                 'northeast_year2004','midwest_year2000', 'midwest_year2001', 'midwest_year2002', \n",
    "                 'midwest_year2003', 'midwest_year2004','south_year2000', 'south_year2001', 'south_year2002', \n",
    "                 'south_year2003', 'south_year2004','west_year2000', 'west_year2001', 'west_year2002', \n",
    "                 'west_year2003']\n",
    "\n",
    "# Deviation Specification - Future Steps\n",
    "selectedvariables_deviation = ['deviation','heat','skycover', 'co', 'co_distance', 'ozone', 'ozone_distance', 'pm', \n",
    "                     'pm_distance', 'press6t4', 'dew6t4', 'prcp6t4', 'wind6t4', \n",
    "                     'rh6t4', 'chair', 'dummy_asylum', 'dummy_gender', \n",
    "                     'middleast', 'america', 'africa', 'europe', 'northeast', 'midwest', \n",
    "                     'south', 'year2000', 'year2001', 'year2002', \n",
    "                     'year2003','middleast_dev','america_dev','africa_dev','europe_dev','month_1',\n",
    "                     'month_2','month_3','month_4','month_5','month_6','month_7','month_8',\n",
    "                     'month_9','month_10','month_11']\n",
    "\n",
    "all_variables = selectedvariables + year_location\n",
    "                        \n",
    "#Create X variables with different specification\n",
    "X = df_final[selectedvariables]\n",
    "\n",
    "X_no_control = df_final[selectedvariables_noweather]\n",
    "\n",
    "X_all = df_final[all_variables]\n",
    "\n",
    "X_deviation = df_final[selectedvariables_deviation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specification 1\n",
    "model_1 = Probit(Y, X.astype(float))\n",
    "probit_model1 = model_1.fit()\n",
    "print(probit_model1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate predicted probabilities\n",
    "predicted_probs = probit_model1.predict(X.astype(float))\n",
    "\n",
    "# Calculate marginal effect for the variable of interest\n",
    "x_temp6t4 = X['temp6t4']  \n",
    "marginal_effect_temp = probit_model1.params['temp6t4'] * predicted_probs * (1 - predicted_probs)\n",
    "average_marginal_effect_temp = np.mean(marginal_effect_temp)\n",
    "\n",
    "print(f\"Average Marginal Effect: {average_marginal_effect_temp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass your fitted Probit model to Stargazer\n",
    "stargazer = Stargazer([probit_model1])\n",
    "\n",
    "# Output as LaTeX\n",
    "print(stargazer.render_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specification 1 - Deviation - Future Steps\n",
    "\n",
    "model_1_deviation = Probit(Y, X_deviation.astype(float))\n",
    "probit_model1_deviation = model_1_deviation.fit()\n",
    "print(probit_model1_deviation.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stargazer_dev = Stargazer([probit_model1_deviation])\n",
    "# Output as LaTeX\n",
    "print(stargazer_dev.render_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specification 2\n",
    "\n",
    "model_2 = Probit(Y, X_no_control.astype(float))\n",
    "probit_model2 = model_2.fit()\n",
    "print(probit_model2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass your fitted Probit model to Stargazer\n",
    "stargazer_2 = Stargazer([probit_model2])\n",
    "\n",
    "# Output as LaTeX\n",
    "print(stargazer_2.render_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate predicted probabilities\n",
    "predicted_probs = probit_model2.predict(X_no_control.astype(float))\n",
    "\n",
    "# Calculate marginal effect for the variable of interest \n",
    "x_temp6t4 = X['temp6t4']  \n",
    "marginal_effect_temp = probit_model2.params['temp6t4'] * predicted_probs * (1 - predicted_probs)\n",
    "\n",
    "# Average marginal effect\n",
    "average_marginal_effect_temp = np.mean(marginal_effect_temp)\n",
    "\n",
    "print(f\"Average Marginal Effect: {average_marginal_effect_temp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specification 3 - Additional Fixed Effects - Not included in the report\n",
    "model_3 = Probit(Y, X_all.astype(float))\n",
    "probit_model3 = model_3.fit()\n",
    "print(probit_model3.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specification - Future Steps - Per region\n",
    "#Change the df_final['location'] for each region of interest\n",
    "\n",
    "df_filtered_location = df_final[df_final['west'] == 1]\n",
    "\n",
    "Y_location = np.array(df_filtered_location['res'])  \n",
    "\n",
    "#Remove the location fixed effects as the filtered dataset only includes one region\n",
    "selectedvariables = ['temp6t4','heat','skycover', 'co', 'co_distance', 'ozone', 'ozone_distance', 'pm', \n",
    "                     'pm_distance', 'press6t4', 'dew6t4', 'prcp6t4', 'wind6t4', \n",
    "                     'rh6t4', 'chair', 'dummy_asylum', 'dummy_gender', \n",
    "                     'middleast', 'america', 'africa', 'europe', 'year2000', 'year2001', 'year2002', \n",
    "                     'year2003','middleast_dev','america_dev','africa_dev','europe_dev','month_1',\n",
    "                     'month_2','month_3','month_4','month_5','month_6','month_7','month_8',\n",
    "                     'month_9','month_10','month_11']\n",
    "\n",
    "X_location = df_filtered_location[selectedvariables]  \n",
    "\n",
    "#Run specification 1 with filtered dataset\n",
    "model_4_location = Probit(Y_location, X_location.astype(float))\n",
    "probit_model4_location = model_4_location.fit()\n",
    "print(probit_model4_location.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stargazer_location = Stargazer([probit_model4_location])\n",
    "# Output as LaTeX\n",
    "print(stargazer_location.render_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame per months (winter vs non. winter months)\n",
    "\n",
    "df_final['date'] = pd.to_datetime(df_final['date'])\n",
    "\n",
    "#Filter the dataset to include only months we want (grouping fall with winter and summer and spring)\n",
    "df_filter_nowinter = df_final[df_final['date'].dt.month.isin([3, 4, 5, 6, 7, 8])]\n",
    "df_filter_winter = df_final[df_final['date'].dt.month.isin([1,2,9,10,11,12])]\n",
    "\n",
    "# Define selected variables for no winter, drop winter months dummies\n",
    "selectedvariables_nowinter = ['skycover', 'co', 'co_distance', 'ozone', 'ozone_distance', 'pm', \n",
    "                     'pm_distance', 'temp6t4','deviation', 'press6t4', 'dew6t4', 'prcp6t4', 'wind6t4', \n",
    "                     'rh6t4', 'heat', 'chair', 'dummy_asylum', 'dummy_gender', \n",
    "                     'middleast', 'america', 'africa', 'europe', 'northeast', 'midwest', \n",
    "                     'south', 'year2000', 'year2001', 'year2002', \n",
    "                     'year2003','middleast_dev','america_dev','africa_dev','europe_dev','month_3',\n",
    "                     'month_4','month_5','month_6','month_7','month_8']\n",
    "\n",
    "# Define selected variables for winter, drop non-winter months dummies\n",
    "selectedvariables_winter = ['skycover', 'co', 'co_distance', 'ozone', 'ozone_distance', 'pm', \n",
    "                     'pm_distance', 'temp6t4','deviation', 'press6t4', 'dew6t4', 'prcp6t4', 'wind6t4', \n",
    "                     'rh6t4', 'heat', 'chair', 'dummy_asylum', 'dummy_gender', \n",
    "                     'middleast', 'america', 'africa', 'europe', 'northeast', 'midwest', \n",
    "                     'south', 'year2000', 'year2001', 'year2002', \n",
    "                     'year2003','middleast_dev','america_dev','africa_dev','europe_dev','month_1','month_2',\n",
    "                    'month_9','month_10']\n",
    "\n",
    "\n",
    "#Create X and Y variables\n",
    "Y_nowinter = np.array(df_filter_nowinter['res'])  \n",
    "X_nowinter = df_filter_nowinter[selectedvariables_nowinter]  \n",
    "\n",
    "Y_winter = np.array(df_filter_winter['res'])  \n",
    "X_winter = df_filter_winter[selectedvariables_winter]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.family'] = 'DejaVu Sans'  \n",
    "\n",
    "# Calculate the average and standard deviation for winter and non-winter months\n",
    "winter_mean = df_filter_winter['res'].mean()\n",
    "winter_std = df_filter_winter['res'].std()\n",
    "nowinter_mean = df_filter_nowinter['res'].mean()\n",
    "nowinter_std = df_filter_nowinter['res'].std()\n",
    "\n",
    "# Prepare data for the bar plot\n",
    "summary = pd.DataFrame({\n",
    "    'Season': ['Winter and Fall', 'Summer and Spring'],\n",
    "    'Mean': [winter_mean, nowinter_mean],\n",
    "    'Std Dev': [winter_std, nowinter_std]\n",
    "})\n",
    "\n",
    "print(winter_mean)\n",
    "print(winter_std)\n",
    "print(nowinter_mean)\n",
    "print(nowinter_std)\n",
    "\n",
    "# Create the bar plot with error bars\n",
    "plt.figure(figsize=(7, 7), facecolor='white')\n",
    "ax = plt.subplot(111, facecolor='white')\n",
    "ax.grid(False)  # Turn off gridlines\n",
    "\n",
    "\n",
    "# Plot bars with error bars\n",
    "bars = plt.bar(summary['Season'], summary['Mean'], yerr=summary['Std Dev'], capsize=5, color=['skyblue', 'lightcoral'])\n",
    "\n",
    "# Add labels and title\n",
    "plt.ylabel('Average Resolution')\n",
    "\n",
    "# Remove plot spines for a clean look\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_visible(False)\n",
    "\n",
    "# Show the plot\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model with dataframe filtered with no winter months - Future Steps\n",
    "\n",
    "model_5 = Probit(Y_nowinter, X_nowinter.astype(float))\n",
    "probit_model5 = model_5.fit()\n",
    "print(probit_model5.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stargazer_non_winter = Stargazer([probit_model5])\n",
    "# Output as LaTeX\n",
    "print(stargazer_non_winter.render_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model with dataframe filtered with only winter months - Future Steps\n",
    "\n",
    "model_6 = Probit(Y_winter, X_winter.astype(float))\n",
    "probit_model6 = model_6.fit()\n",
    "print(probit_model6.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stargazer_winter = Stargazer([probit_model6])\n",
    "# Output as LaTeX\n",
    "print(stargazer_winter.render_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most interesting finding is that temperature has a positive coefficient in our logit model (opposite to the original paper) and almost 0 in the Ridge model. Meanwhile, in the Lasso model temperature has a negative coefficient in line with the original paper. Perhaps when more variables are included the effect of temperature is diminished. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Ridge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_names = {\n",
    "    'temp6t4': 'Temperature',\n",
    "    'middleast_dev': 'Middle East*Temperature',\n",
    "    'america_dev': 'America*Temperature',\n",
    "    'africa_dev': 'Africa*Temperature',\n",
    "    'europe_dev': 'Europe*Temperature'\n",
    "}\n",
    "\n",
    "# Rename the columns in df_final\n",
    "df_final_renamed = df_final.rename(columns=new_names)\n",
    "\n",
    "# Now select the renamed variables for X_Ridge\n",
    "selected_variables_nofe = ['Temperature', 'Middle East*Temperature', 'America*Temperature', 'Africa*Temperature', 'Europe*Temperature']\n",
    "X_Ridge = df_final_renamed[selected_variables_nofe]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run a Ridge Model\n",
    "#Code in this section based on the notebook: ISLP-Ch06_varselect_lab.ipynb\n",
    "\n",
    "coefficients = []\n",
    "#Calculate lambda from 10^8 to 10^-2\n",
    "lambdas = 10**np.linspace(8, -2, 100) / Y.std()\n",
    "scaler = StandardScaler(with_mean=True,  with_std=True)\n",
    "for lam in lambdas:\n",
    "    ridge = SGDClassifier(loss='log_loss', penalty='l2',alpha=lam)\n",
    "    # Create a pipeline with scaling and the classifier\n",
    "    pipe = Pipeline(steps=[('scaler', scaler), ('ridge', ridge)])\n",
    "    # Fit the pipeline to the data\n",
    "    pipe.fit(X_Ridge, Y)\n",
    "    # Store the coefficients \n",
    "    coefficients.append(pipe.named_steps['ridge'].coef_.flatten())\n",
    "\n",
    "#solution containing all our coefficients    \n",
    "soln_array = np.array(coefficients)\n",
    "\n",
    "# Create a DataFrame with the solution path, for easy  transposing soln_array so features are in columns\n",
    "soln_path = pd.DataFrame(soln_array, columns=X_Ridge.columns, index=-np.log(lambdas))\n",
    "# Name the index to indicate it's the negative log of lambda\n",
    "soln_path.index.name = 'negative log(lambda)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the graph  \n",
    "path_fig, ax = subplots(figsize=(8,8))\n",
    "soln_path.plot(ax=ax, legend=False)\n",
    "ax.set_xlabel('$-\\log(\\lambda)$', fontsize=20)\n",
    "ax.set_ylabel('Standardized coefficients', fontsize=20)\n",
    "ax.legend(loc='upper left');\n",
    "\n",
    "path_fig.patch.set_facecolor('white')  \n",
    "ax.set_facecolor('white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize K-Fold cross-validation strategy\n",
    "kfold = KFold(n_splits=5)\n",
    "\n",
    "param_grid = {'ridge__alpha': lambdas}\n",
    "\n",
    "# Use GridSearchCV to find the best alpha using cross-validation\n",
    "#Since we have a categorical variable the scoring is accuracy (where it defines how accurate we predict y)\n",
    "grid_search = GridSearchCV(pipe, param_grid, cv=kfold, scoring='accuracy', return_train_score=True)\n",
    "\n",
    "# Fit the model using cross-validation to find the best alpha\n",
    "grid_search.fit(X_Ridge, Y)\n",
    "\n",
    "# Find optimal lambda\n",
    "tuned_ridge = grid_search.best_estimator_.named_steps['ridge']\n",
    "\n",
    "# Retrieve the mean and standard deviation of cross-validation scores\n",
    "mean_scores = grid_search.cv_results_['mean_test_score']\n",
    "std_scores = grid_search.cv_results_['std_test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the Cross-Validated Accuracy and Error Bars\n",
    "ridgeCV_fig, ax = subplots(figsize=(8, 8))\n",
    "ax.errorbar(-np.log(lambdas), mean_scores, yerr=std_scores / np.sqrt(kfold.get_n_splits()), fmt='o')\n",
    "ax.axvline(-np.log(grid_search.best_params_['ridge__alpha']), c='k', ls='--')\n",
    "ax.set_xlabel('$-\\log(\\\\lambda)$', fontsize=20)\n",
    "ax.set_ylabel('Cross-validated Accuracy', fontsize=20)\n",
    "ax.set_title('Cross-Validation Accuracy with Error Bars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = tuned_ridge.coef_.flatten()  \n",
    "variable_names = X_Ridge.columns  # Get the names of the features\n",
    "\n",
    "# Create a dictionary mapping variable names to their coefficients\n",
    "coef_mapping = {variable: coef for variable, coef in zip(variable_names, coefficients)}\n",
    "\n",
    "# Convert the dictionary to a DataFrame for better visualization\n",
    "coef_df = pd.DataFrame(list(coef_mapping.items()), columns=['Variable', 'Coefficient'])\n",
    "print(coef_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code in this section based on the notebook: ISLP-Ch06_varselect_lab.ipynb\n",
    "\n",
    "#Running Lasso\n",
    "coefficients_l = []\n",
    "for lam in lambdas:\n",
    "    lasso = SGDClassifier(loss='log_loss', penalty='l1',alpha=lam)\n",
    "    # Create a pipeline with scaling and the classifier\n",
    "    pipe_l = Pipeline(steps=[('scaler', scaler), ('lasso', lasso)])\n",
    "    # Fit the pipeline to the data\n",
    "    pipe_l.fit(X, Y)\n",
    "    # Store the coefficients \n",
    "    coefficients_l.append(pipe_l.named_steps['lasso'].coef_.flatten())\n",
    "\n",
    "soln_array_l = np.array(coefficients_l)\n",
    "# Create a DataFrame with the solution path, transposing soln_array so features are in columns\n",
    "soln_path_l = pd.DataFrame(soln_array_l, columns=X.columns, index=-np.log(lambdas))\n",
    "# Name the index to indicate it's the negative log of lambda\n",
    "soln_path_l.index.name = 'negative log(lambda)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_fig_l, ax = subplots(figsize=(8,8))\n",
    "soln_path_l.plot(ax=ax, legend=False)\n",
    "ax.set_xlabel('$-\\log(\\lambda)$', fontsize=20)\n",
    "ax.set_ylabel('Standardized coefficients', fontsize=20)\n",
    "ax.legend(loc='upper left');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=5)\n",
    "\n",
    "# Define a grid of alpha values to search over\n",
    "param_grid_l = {'lasso__alpha': lambdas}\n",
    "\n",
    "# Use GridSearchCV to find the best alpha using cross-validation\n",
    "# Since we have a categorical variable the scoring is not MSE but accuracy\n",
    "grid_search_l = GridSearchCV(pipe_l, param_grid_l, cv=kfold, scoring='accuracy', return_train_score=True)\n",
    "\n",
    "# Fit the model using cross-validation to find the best alpha\n",
    "grid_search_l.fit(X, Y)\n",
    "\n",
    "# Get the tuned LASSO model (SGDClassifier)\n",
    "tuned_lasso = grid_search_l.best_estimator_.named_steps['lasso']\n",
    "\n",
    "# Retrieve the mean and standard deviation of cross-validation scores\n",
    "mean_scores_l = grid_search_l.cv_results_['mean_test_score']\n",
    "std_scores_l = grid_search_l.cv_results_['std_test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the Cross-Validated Accuracy and Error Bars\n",
    "lassoCV_fig, ax = subplots(figsize=(8, 8))\n",
    "ax.errorbar(-np.log(lambdas), mean_scores_l, yerr=std_scores_l / np.sqrt(kfold.get_n_splits()), fmt='o')\n",
    "ax.axvline(-np.log(grid_search_l.best_params_['lasso__alpha']), c='k', ls='--')\n",
    "ax.set_xlabel('$-\\log(\\\\lambda)$', fontsize=20)\n",
    "ax.set_ylabel('Cross-validated Accuracy', fontsize=20)\n",
    "ax.set_title('Cross-Validation Accuracy with Error Bars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients_l = tuned_lasso.coef_.flatten()  \n",
    "variable_names = X.columns  # Get the names of the features\n",
    "\n",
    "# Create a dictionary mapping variable names to their coefficients\n",
    "coef_mapping_l = {variable: coef for variable, coef in zip(variable_names, coefficients_l)}\n",
    "\n",
    "# Convert the dictionary to a DataFrame for better visualization\n",
    "coef_df = pd.DataFrame(list(coef_mapping_l.items()), columns=['Variable', 'Coefficient'])\n",
    "print(coef_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresion Trees "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code in this section based on the notebook: ISLP-TreeModels.ipynb\n",
    "\n",
    "clf = DTC(criterion='entropy', \n",
    "          max_depth = 3,\n",
    "          random_state=0)   \n",
    "\n",
    "clf.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify columns\n",
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename columns for better labelling of trees\n",
    "X_detailed = ['Average temperature', 'Heat index', 'Sky coverage', 'Carbon monoxide levels', \n",
    "              'Distance CO source', 'Ozone levels', 'Distance ozone source', 'PM levels', \n",
    "              'Distance PM source', 'Atmospheric pressure', 'Dew point temperature', 'Precipitation', \n",
    "              'Wind speed', 'Relative humidity', 'Judge identifier', 'Asylum application', \n",
    "              'Gender', 'Middle Eastern', 'American', 'African', 'European', 'Northeast', 'Midwest', 'South', \n",
    "              '2000','2001', '2002', '2003', 'Interaction of temperature and Middle Eastern',\n",
    "              'Interaction of temperature and American', 'Interaction of temperature and African', \n",
    "              'Interaction of temperature and European', 'January', 'February', 'March', 'April', 'May', 'June', \n",
    "              'July', 'August', 'September', 'October', 'November']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = X_detailed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(Y, clf.predict(X)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "\n",
    "# Remove the gray background\n",
    "fig.patch.set_facecolor('white')  \n",
    "ax.set_facecolor('white')        \n",
    "\n",
    "plot_tree(clf,\n",
    "          feature_names=feature_names,\n",
    "          ax=ax,\n",
    "          filled=True,  \n",
    "          rounded=True,  \n",
    "          fontsize=7,   \n",
    "          proportion=True)  \n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross validation \n",
    "validation = skm.ShuffleSplit(n_splits=1,\n",
    "                              test_size=200,\n",
    "                              random_state=0)\n",
    "results = skm.cross_validate(clf,\n",
    "                             X,\n",
    "                             Y,\n",
    "                             cv=validation)\n",
    "results['test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split dataset\n",
    "(X_train,\n",
    " X_test,\n",
    " Y_train,\n",
    " Y_test) = skm.train_test_split(X,\n",
    "                                   Y,\n",
    "                                   test_size=0.5,\n",
    "                                   random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DTC(criterion='entropy', random_state=0)\n",
    "clf.fit(X_train, Y_train)\n",
    "accuracy_score(Y_test, clf.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccp_path = clf.cost_complexity_pruning_path(X_train, Y_train)\n",
    "kfold = skm.KFold(5,\n",
    "                  random_state=1,\n",
    "                  shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code in this section based on the notebook: ISLP-TreeModels.ipynb\n",
    "\n",
    "bag_temperature = RFC(max_features=X_train.shape[1], random_state=0)\n",
    "bag_temperature.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_temperature = RFC(max_features=X_train.shape[1],\n",
    "                n_estimators=500,#how many trees you are running\n",
    "                random_state=0).fit(X_train, Y_train)\n",
    "y_hat_bag = bag_temperature.predict(X_test)\n",
    "accuracy_bagging = accuracy_score(Y_test, y_hat_bag)\n",
    "accuracy_bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_imp_bag = pd.DataFrame(\n",
    "    {'importance':bag_temperature.feature_importances_},\n",
    "    index=feature_names)\n",
    "feature_imp_bag.sort_values(by='importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_imp_bag = pd.DataFrame(\n",
    "    {'importance': bag_temperature.feature_importances_},\n",
    "    index=feature_names\n",
    ")\n",
    "\n",
    "# Sort the feature importances\n",
    "feature_imp_bag = feature_imp_bag.sort_values(by='importance', ascending=False)\n",
    "\n",
    "# Plot the feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_imp_bag.index, feature_imp_bag['importance'], color='skyblue')\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Feature Importances (Bagged Model)')\n",
    "plt.gca().invert_yaxis()  # To display the most important feature at the top\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code in this section based on the notebook: ISLP-TreeModels.ipynb\n",
    "\n",
    "RF_temperature = RFC(max_features=6,\n",
    "               random_state=0).fit(X_train, Y_train)\n",
    "y_hat_RF = RF_temperature.predict(X_test)\n",
    "accuracy_RF = accuracy_score(Y_test, y_hat_RF)\n",
    "accuracy_RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_imp = pd.DataFrame(\n",
    "    {'importance':RF_temperature.feature_importances_},\n",
    "    index=feature_names)\n",
    "feature_imp.sort_values(by='importance', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code in this section based on the notebook: ISLP-TreeModels.ipynb\n",
    "\n",
    "boost_temperature = GBC(n_estimators=500,\n",
    "                   learning_rate=0.001,\n",
    "                    max_depth = 3,\n",
    "                   random_state=0)\n",
    "boost_temperature.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boost_temperature = GBC(n_estimators=500,\n",
    "                   learning_rate=0.001,\n",
    "                    max_depth = 3,\n",
    "                   random_state=0)\n",
    "boost_temperature.fit(X_train,Y_train)\n",
    "y_hat_boost = boost_temperature.predict(X_test);\n",
    "accuracy_boosting = accuracy_score(Y_test, y_hat_boost)\n",
    "accuracy_boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_imp = pd.DataFrame(\n",
    "    {'importance':boost_temperature.feature_importances_},\n",
    "    index=feature_names)\n",
    "feature_imp.sort_values(by='importance', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directed Acyclic Graph (DAG) and Causal Relationship "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code in this section based on the notebook: CIDP-Chapter_04\n",
    "\n",
    "# Define the graph\n",
    "sample_gml = \"\"\"graph [\n",
    "directed 1\n",
    "\n",
    "node [\n",
    "    id 1\n",
    "    label \"Cognitive\"\n",
    "    ]\n",
    "    \n",
    "node [\n",
    "    id 2\n",
    "    label \"Weather\"\n",
    "    ]\n",
    "\n",
    "node [\n",
    "    id 4\n",
    "    label \"Resolution\"\n",
    "    ]\n",
    "    \n",
    "node [\n",
    "    id 5\n",
    "    label \"Location\"\n",
    "    ]\n",
    "\n",
    "node [\n",
    "    id 6\n",
    "    label \"Judge\"\n",
    "    ]\n",
    "\n",
    "node [\n",
    "    id 7\n",
    "    label \"Nationality\"\n",
    "    ]\n",
    "    \n",
    "edge [\n",
    "    source 2\n",
    "    target 1\n",
    "    ]\n",
    "\n",
    "edge [\n",
    "    source 5\n",
    "    target 4\n",
    "    ]\n",
    "\n",
    "edge [\n",
    "    source 6\n",
    "    target 4\n",
    "    ]\n",
    "\n",
    "edge [\n",
    "    source 1\n",
    "    target 6\n",
    "    ]\n",
    "edge [\n",
    "    source 7\n",
    "    target 4\n",
    "    ]\n",
    "edge [\n",
    "    source 5\n",
    "    target 2\n",
    "    ]\n",
    "]\n",
    "    \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the graph\n",
    "graph = nx.parse_gml(sample_gml)\n",
    "\n",
    "# Plot\n",
    "nx.draw(\n",
    "    G=graph, \n",
    "    with_labels=True,\n",
    "    node_size=2500,\n",
    "    node_color=COLORS[0],\n",
    "    font_color='black',\n",
    "    font_size = 8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the graph\n",
    "sample_gml2 = \"\"\"graph [\n",
    "directed 1\n",
    "\n",
    "node [\n",
    "    id 1\n",
    "    label \"Cognitive\"\n",
    "    ]\n",
    "    \n",
    "node [\n",
    "    id 2\n",
    "    label \"Weather\"\n",
    "    ]\n",
    "\n",
    "node [\n",
    "    id 4\n",
    "    label \"Resolution\"\n",
    "    ]\n",
    "    \n",
    "node [\n",
    "    id 5\n",
    "    label \"Location\"\n",
    "    ]\n",
    "\n",
    "node [\n",
    "    id 6\n",
    "    label \"Judge\"\n",
    "    ]\n",
    "\n",
    "node [\n",
    "    id 7\n",
    "    label \"Nationality\"\n",
    "    ]\n",
    "    \n",
    "edge [\n",
    "    source 2\n",
    "    target 1\n",
    "    ]\n",
    "\n",
    "edge [\n",
    "    source 5\n",
    "    target 4\n",
    "    ]\n",
    "\n",
    "edge [\n",
    "    source 6\n",
    "    target 4\n",
    "    ]\n",
    "\n",
    "edge [\n",
    "    source 1\n",
    "    target 6\n",
    "    ]\n",
    "edge [\n",
    "    source 7\n",
    "    target 1\n",
    "    ]\n",
    "edge [\n",
    "    source 5\n",
    "    target 2\n",
    "    ]\n",
    "]\n",
    "    \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the graph\n",
    "graph = nx.parse_gml(sample_gml2)\n",
    "\n",
    "# Plot\n",
    "nx.draw(\n",
    "    G=graph, \n",
    "    with_labels=True,\n",
    "    node_size=2500,\n",
    "    node_color=COLORS[0],\n",
    "    font_color='black',\n",
    "    font_size = 8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the graph\n",
    "sample_gml3 = \"\"\"graph [\n",
    "directed 1\n",
    "\n",
    "node [\n",
    "    id 1\n",
    "    label \"Cognitive\"\n",
    "    ]\n",
    "    \n",
    "node [\n",
    "    id 2\n",
    "    label \"Weather\"\n",
    "    ]\n",
    "\n",
    "node [\n",
    "    id 4\n",
    "    label \"Resolution\"\n",
    "    ]\n",
    "    \n",
    "node [\n",
    "    id 5\n",
    "    label \"Location\"\n",
    "    ]\n",
    "\n",
    "node [\n",
    "    id 6\n",
    "    label \"Judge\"\n",
    "    ]\n",
    "\n",
    "node [\n",
    "    id 7\n",
    "    label \"Nationality\"\n",
    "    ]\n",
    "    \n",
    "edge [\n",
    "    source 2\n",
    "    target 1\n",
    "    ]\n",
    "\n",
    "edge [\n",
    "    source 6\n",
    "    target 4\n",
    "    ]\n",
    "\n",
    "edge [\n",
    "    source 1\n",
    "    target 6\n",
    "    ]\n",
    "edge [\n",
    "    source 7\n",
    "    target 1\n",
    "    ]\n",
    "edge [\n",
    "    source 5\n",
    "    target 2\n",
    "    ]\n",
    "edge [\n",
    "    source 5\n",
    "    target 6\n",
    "    ]\n",
    "]\n",
    "    \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the graph\n",
    "graph = nx.parse_gml(sample_gml3)\n",
    "\n",
    "# Plot\n",
    "nx.draw(\n",
    "    G=graph, \n",
    "    with_labels=True,\n",
    "    node_size=2500,\n",
    "    node_color=COLORS[0],\n",
    "    font_color='black',\n",
    "    font_size = 8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the graph\n",
    "gml_final = \"\"\"graph [\n",
    "directed 1\n",
    "    \n",
    "node [\n",
    "    id 1\n",
    "    label \"midwest\"\n",
    "    ]\n",
    "\n",
    "node [\n",
    "    id 2\n",
    "    label \"deviation\"\n",
    "    ]\n",
    "\n",
    "node [\n",
    "    id 4\n",
    "    label \"res\"\n",
    "    ]\n",
    "    \n",
    "node [\n",
    "    id 5\n",
    "    label \"northeast\"\n",
    "    ]\n",
    "\n",
    "node [\n",
    "    id 6\n",
    "    label \"chair\"\n",
    "    ]\n",
    "\n",
    "node [\n",
    "    id 7\n",
    "    label \"america\"\n",
    "    ]\n",
    "node [\n",
    "    id 8\n",
    "    label \"south\"\n",
    "    ]\n",
    "node [\n",
    "    id 9\n",
    "    label \"west\"\n",
    "    ]\n",
    "node [\n",
    "    id 11\n",
    "    label \"cognitive\"\n",
    "    ]\n",
    "    \n",
    "edge [\n",
    "    source 2\n",
    "    target 11\n",
    "    ]\n",
    "\n",
    "edge [\n",
    "    source 6\n",
    "    target 4\n",
    "    ]\n",
    "\n",
    "edge [\n",
    "    source 7\n",
    "    target 11\n",
    "    ]\n",
    "edge [\n",
    "    source 5\n",
    "    target 2\n",
    "    ]\n",
    "edge [\n",
    "    source 1\n",
    "    target 2\n",
    "    ]\n",
    "edge [\n",
    "    source 8\n",
    "    target 2\n",
    "    ]\n",
    "edge [\n",
    "    source 9\n",
    "    target 2\n",
    "    ]\n",
    "edge [\n",
    "    source 5\n",
    "    target 11\n",
    "    ]\n",
    "edge [\n",
    "    source 1\n",
    "    target 11\n",
    "    ]\n",
    "edge [\n",
    "    source 8\n",
    "    target 11\n",
    "    ]\n",
    "edge [\n",
    "    source 9\n",
    "    target 11\n",
    "    ]\n",
    "edge [ \n",
    "    source 11\n",
    "    target 6\n",
    "    ]\n",
    "]\n",
    "\n",
    "    \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the graph\n",
    "graph = nx.parse_gml(gml_final)\n",
    "\n",
    "# Plot\n",
    "nx.draw(\n",
    "    G=graph, \n",
    "    with_labels=True,\n",
    "    node_size=2500,\n",
    "    node_color=COLORS[0],\n",
    "    font_color='black',\n",
    "    font_size = 8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test DAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code in this section based on the notebook:  CIDP-Chapter_07\n",
    "\n",
    "#Model the problem\n",
    "model = CausalModel(\n",
    "data=df_final,\n",
    "treatment=['deviation'],\n",
    "outcome=\"res\",\n",
    "graph=gml_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify the estimand\n",
    "estimand = model.identify_effect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(estimand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtain estimates\n",
    "estimate = model.estimate_effect(\n",
    "identified_estimand=estimand,\n",
    "method_name=\"backdoor.linear_regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(estimate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform refutation test\n",
    "#Refutation test on whether estimate is influenced by unobserved confounders = random_common_cause \n",
    "refute_subset = model.refute_estimate(\n",
    "estimand=estimand,\n",
    "estimate=estimate,\n",
    "method_name=\"random_common_cause\",\n",
    "subset_fraction=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(refute_subset)\n",
    "#High p-value suuggests that the random common cause does not have a meaningful impact on the relationship between \n",
    "#temperature and the outcome, providing confidence in the stability of findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IPW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code in this section based on the notebook: CIBT-11-Propensity-Score\n",
    "\n",
    "#Heat stress begins at 33 degrees\n",
    "df_final['T_binary'] = (df_final['temp6t4'] > 75).astype(int)\n",
    "print(df_final['T_binary'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 'T_binary'\n",
    "Y = 'res'\n",
    "X = ['chair', 'dummy_asylum', 'dummy_gender', \n",
    "                     'middleast', 'america', 'africa', 'europe', 'northeast', 'midwest', \n",
    "                     'south', 'year2000', 'year2001', 'year2002', \n",
    "                     'year2003','middleast_dev','america_dev','africa_dev','europe_dev','month_1',\n",
    "                     'month_2','month_3','month_4','month_5','month_6','month_7','month_8',\n",
    "                     'month_9','month_10','month_11']\n",
    "\n",
    "#'heat','skycover', 'co', 'co_distance', 'ozone', 'ozone_distance', 'pm', 'pm_distance', 'press6t4', 'dew6t4', 'prcp6t4', 'wind6t4','rh6t4', \n",
    "\n",
    "df_final_limited = df_final[(df_final['temp6t4'] > 60) & (df_final['temp6t4'] < 90)]\n",
    "\n",
    "ps_model = LogisticRegression(C=1e6).fit(df_final_limited[X], df_final_limited[T])\n",
    "\n",
    "data_ps = df_final_limited.assign(propensity_score=ps_model.predict_proba(df_final_limited[X])[:, 1])\n",
    "\n",
    "data_ps[[\"T_binary\", \"res\", \"propensity_score\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_t = 1/data_ps.query(\"T_binary==1\")[\"propensity_score\"]\n",
    "weight_nt = 1/(1-data_ps.query(\"T_binary==0\")[\"propensity_score\"])\n",
    "print(\"Original Sample Size\", df.shape[0])\n",
    "print(\"Treated Population Sample Size\", sum(weight_t))\n",
    "print(\"Untreated Population Sample Size\", sum(weight_nt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(data_ps.query(\"T_binary==0\")[\"propensity_score\"], kde=False, label=\"Non Treated\")\n",
    "sns.distplot(data_ps.query(\"T_binary==1\")[\"propensity_score\"], kde=False, label=\"Treated\")\n",
    "plt.title(\"Positivity Check\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove observations with propensity score = 1\n",
    "data_ps = data_ps[data_ps[\"propensity_score\"] < 1]\n",
    "\n",
    "\n",
    "# Calculate weights for treated and control groups\n",
    "treated_data = data_ps.query(\"T_binary == 1\")\n",
    "control_data = data_ps.query(\"T_binary == 0\")\n",
    "\n",
    "# Calculate weighted outcomes for treated (Y1) and control (Y0)\n",
    "y1 = sum(treated_data[\"T_binary\"] * weight_t) / len(treated_data)\n",
    "y0 = sum(control_data[\"T_binary\"] * weight_nt) / len(control_data)\n",
    "\n",
    "# Calculate the ATE (Average Treatment Effect)\n",
    "ate = np.mean(weight_t * treated_data[\"T_binary\"]) - np.mean(weight_nt * control_data[\"T_binary\"])\n",
    "\n",
    "# Print results\n",
    "print(\"Y1:\", y1)\n",
    "print(\"Y0:\", y0)\n",
    "print(\"ATE:\", ate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate weights for treated and control groups\n",
    "treated_data = data_ps.query(\"res == 1\")\n",
    "control_data = data_ps.query(\"res == 0\")\n",
    "\n",
    "if not treated_data.empty:\n",
    "    weight_t = 1 / treated_data[\"propensity_score\"]\n",
    "    print(\"Weight_t:\", weight_t)\n",
    "\n",
    "if not control_data.empty:\n",
    "    weight_nt = 1 / (1 - control_data[\"propensity_score\"])\n",
    "    print(\"Weight_nt:\", weight_nt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_ps[\"propensity_score\"].min(), data_ps[\"propensity_score\"].max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ps(df_final, X, T, y):\n",
    "    # estimate the propensity score\n",
    "    ps = LogisticRegression(C=1e6, max_iter=2000, solver='liblinear').fit(df_final[X], df_final[T]).predict_proba(df_final[X])[:, 1]\n",
    "    weight = (df_final[T]-ps) / (ps*(1-ps)) # define the weights\n",
    "    return np.mean(weight * df_final[y]) # compute the ATE\n",
    "\n",
    "sample_df = df_final.sample(frac=1, replace=True)\n",
    "ate = run_ps(sample_df, X, T, Y)\n",
    "print(ate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed # for parallel processing\n",
    "\n",
    "sample_df = df_final.sample(frac=0.5, replace=True)\n",
    "\n",
    "# define function that computes the IPTW estimator\n",
    "def run_ps(sample_df, X, T, y):\n",
    "    # estimate the propensity score\n",
    "    ps = LogisticRegression(C=1e6, max_iter=2000, solver='liblinear').fit(sample_df[X], sample_df[T]).predict_proba(sample_df[X])[:, 1]\n",
    "    weight = (sample_df[T]-ps) / (ps*(1-ps)) # define the weights\n",
    "    return np.mean(weight * df_final[y]) # compute the ATE\n",
    "\n",
    "np.random.seed(88)\n",
    "# run 100 bootstrap samples\n",
    "bootstrap_sample = 100\n",
    "ates = Parallel(n_jobs=4)(delayed(run_ps)(df_final.sample(frac=1, replace=True), X, T, Y)\n",
    "                          for _ in range(bootstrap_sample))\n",
    "ates = np.array(ates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learners "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code in this section based on the notebook: CIBT-21 Meta-Learners\n",
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split df_final randomly into two parts (e.g., 50% - 50%)\n",
    "df_train, df_test = train_test_split(df_final, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "np.random.seed(123)\n",
    "s_learner = LGBMRegressor(max_depth=3, min_child_samples=30)\n",
    "s_learner.fit(df_train[X+[T]], df_train[Y]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_learner_cate_train = (s_learner.predict(df_train[X].assign(**{T: 1})) -\n",
    "                        s_learner.predict(df_train[X].assign(**{T: 0})))\n",
    "\n",
    "s_learner_cate_test = df_test.assign(\n",
    "    cate=(s_learner.predict(df_test[X].assign(**{T: 1})) - # predict under treatment\n",
    "          s_learner.predict(df_test[X].assign(**{T: 0}))) # predict under control\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elast(data, y, t):\n",
    "    return (np.sum((data[t] - data[t].mean())*(data[y] - data[y].mean())) /\n",
    "            np.sum((data[t] - data[t].mean())**2))\n",
    "\n",
    "def cumulative_gain(dataset, prediction, y, t, min_periods=30, steps=100):\n",
    "    size = dataset.shape[0]\n",
    "    ordered_df = dataset.sort_values(prediction, ascending=False).reset_index(drop=True)\n",
    "    n_rows = list(range(min_periods, size, size // steps)) + [size]\n",
    "    return np.array([elast(ordered_df.head(rows), y, t) * (rows/size) for rows in n_rows])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gain_curve_test = cumulative_gain(s_learner_cate_test, \"cate\", y=\"res\", t=\"T_binary\")\n",
    "gain_curve_train = cumulative_gain(df_train.assign(cate=s_learner_cate_train), \"cate\",y=\"res\", t=\"T_binary\")\n",
    "plt.plot(gain_curve_test, color=\"C0\", label=\"Test\")\n",
    "plt.plot(gain_curve_train, color=\"C1\", label=\"Train\")\n",
    "plt.plot([0, 100], [0, elast(df_test, y=\"res\", t=\"T_binary\")], linestyle=\"--\", color=\"black\", label=\"Baseline\")\n",
    "plt.legend()\n",
    "plt.title(\"S-Learner\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "m0 = LGBMRegressor(max_depth=2, min_child_samples=60)\n",
    "m1 = LGBMRegressor(max_depth=2, min_child_samples=60)\n",
    "\n",
    "m0.fit(df_train.query(f\"{T}==0\")[X], df_train.query(f\"{T}==0\")[Y])\n",
    "m1.fit(df_train.query(f\"{T}==1\")[X], df_train.query(f\"{T}==1\")[Y])\n",
    "\n",
    "# estimate the CATE\n",
    "t_learner_cate_train = m1.predict(df_train[X]) - m0.predict(df_train[X])\n",
    "t_learner_cate_test = df_test.assign(cate=m1.predict(df_test[X]) - m0.predict(df_test[X]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gain_curve_test = cumulative_gain(t_learner_cate_test, \"cate\", y=\"res\", t=\"T_binary\")\n",
    "gain_curve_train = cumulative_gain(df_train.assign(cate=t_learner_cate_train), \"cate\", y=\"res\", t=\"T_binary\")\n",
    "plt.plot(gain_curve_test, color=\"C0\", label=\"Test\")\n",
    "plt.plot(gain_curve_train, color=\"C1\", label=\"Train\")\n",
    "plt.plot([0, 100], [0, elast(df_test, \"res\", \"T_binary\")], linestyle=\"--\", color=\"black\", label=\"Baseline\")\n",
    "plt.legend();\n",
    "plt.title(\"T-Learner\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "# first stage models\n",
    "m0 = LGBMRegressor(max_depth=2, min_child_samples=30)\n",
    "m1 = LGBMRegressor(max_depth=2, min_child_samples=30)\n",
    "\n",
    "# propensity score model\n",
    "g = LogisticRegression(solver=\"lbfgs\", penalty='none') \n",
    "\n",
    "m0.fit(df_train.query(f\"{T}==0\")[X], df_train.query(f\"{T}==0\")[Y])\n",
    "m1.fit(df_train.query(f\"{T}==1\")[X], df_train.query(f\"{T}==1\")[Y])\n",
    "                       \n",
    "g.fit(df_train[X], df_train[T]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train = np.where(df_train[T]==0,\n",
    "                   m1.predict(df_train[X]) - df_train[Y],\n",
    "                   df_train[Y] - m0.predict(df_train[X]))\n",
    "\n",
    "# second stage\n",
    "mx0 = LGBMRegressor(max_depth=2, min_child_samples=30)\n",
    "mx1 = LGBMRegressor(max_depth=2, min_child_samples=30)\n",
    "\n",
    "mx0.fit(df_train.query(f\"{T}==0\")[X], d_train[df_train[T]==0])\n",
    "mx1.fit(df_train.query(f\"{T}==1\")[X], d_train[df_train[T]==1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ps_predict(df, t): \n",
    "    return g.predict_proba(df[X])[:, t]\n",
    "    \n",
    "    \n",
    "x_cate_train = (ps_predict(df_train,1)*mx0.predict(df_train[X]) +\n",
    "                ps_predict(df_train,0)*mx1.predict(df_train[X]))\n",
    "\n",
    "x_cate_test = df_test.assign(cate=(ps_predict(df_test,1)*mx0.predict(df_test[X]) +\n",
    "                                ps_predict(df_test,0)*mx1.predict(df_test[X])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gain_curve_test = cumulative_gain(x_cate_test, \"cate\", y=\"res\", t=\"T_binary\")\n",
    "gain_curve_train = cumulative_gain(df_train.assign(cate=x_cate_train), \"cate\", y=\"res\", t=\"T_binary\")\n",
    "plt.plot(gain_curve_test, color=\"C0\", label=\"Test\")\n",
    "plt.plot(gain_curve_train, color=\"C1\", label=\"Train\")\n",
    "plt.plot([0, 100], [0, elast(df_test, \"res\", \"T_binary\")], linestyle=\"--\", color=\"black\", label=\"Baseline\")\n",
    "plt.legend();\n",
    "plt.title(\"X-Learner\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
