{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data from: Heyes, Anthony, and Soodeh Saberian. 2019. \"Temperature and Decisions: Evidence from 207,000 Court Cases.\" American Economic Journal: Applied Economics, 11 (2): 238â€“65.\n",
    "\n",
    "Notebooks used troughout the code: \n",
    "- ISLP-Ch06_varselect_lab.ipynb\n",
    "- ISLP-TreeModels.ipynb\n",
    "- CIDP-Chapter_04\n",
    "- CIDP-Chapter_05\n",
    "- CIDP-Chapter_07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import sklearn.linear_model as skl\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sklearn.model_selection as skm\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from matplotlib.pyplot import subplots\n",
    "from statsmodels.discrete.discrete_model import Probit\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Tree\n",
    "from sklearn.tree import (DecisionTreeClassifier as DTC,\n",
    "                          DecisionTreeRegressor as DTR,\n",
    "                          plot_tree,\n",
    "                          export_text)\n",
    "from sklearn.metrics import (accuracy_score,\n",
    "                             log_loss)\n",
    "from sklearn.ensemble import \\\n",
    "     (RandomForestClassifier as RFC,\n",
    "      GradientBoostingClassifier as GBC)\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Graphs\n",
    "from scipy import stats\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "import graphviz\n",
    "import networkx as nx\n",
    "COLORS = [\n",
    "    '#00B0F0',\n",
    "    '#FF0000'\n",
    "]\n",
    "import matplotlib.patches as patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DAG\n",
    "!pip install dowhy\n",
    "import dowhy\n",
    "from dowhy import CausalModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Description "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the dataset from the replication package\n",
    "df = pd.read_stata('matched_corrected.dta')\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the list of columns to identify all variables\n",
    "\n",
    "columns_list = df.columns.tolist()\n",
    "print(columns_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dummy for asylum\n",
    "df['dummy_asylum'] = df['c_asy_type'].apply(lambda x: 1 if x == 'E' else 0)\n",
    "#Create a dummy for gender\n",
    "df['dummy_gender'] = df['gender'].apply(lambda x: 1 if x == 'female' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As outlined in the correction article drop the observation for China\n",
    "df = df[df['nat_name'] != 'CHINA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique values to identify variables for the dummy variables\n",
    "unique__names = df['nat_name'].unique()\n",
    "locations = df['location'].unique()\n",
    "\n",
    "#Create a categorical variable for nationatility\n",
    "\n",
    "# Define the list of regions\n",
    "middle_eastern_countries = [\"BAHRAIN\", \"CYPRUS\", \"EGYPT\", \"IRAN\", \"IRAQ\", \"ISRAEL\", \"JORDAN\", \n",
    "    \"KUWAIT\", \"LEBANON\", \"OMAN\", \"PALESTINE\", \"QATAR\", \"SAUDI ARABIA\", \n",
    "    \"SYRIA\", \"TURKEY\", \"UNITED ARAB EMIRATES\", \"YEMEN\"]\n",
    "\n",
    "africa = [\"ERITREA\", \"RWANDA\", \"SOMALIA\", \"SUDAN\", \"CONGO\", \"ETHIOPIA\", \"LIBYA\", \n",
    "    \"MALI\", \"ANGOLA\", \"BURUNDI\", \"TANZANIA\", \"NIGERIA\", \"GABON\", \"GHANA\", \n",
    "    \"SENEGAL\", \"CHAD\", \"DJIBOUTI\", \"CAMEROON\", \"UGANDA\", \"KENYA\", \n",
    "    \"ZAMBIA\", \"MAURITANIA\", \"SOUTH AFRICA\", \"GUINEA\", \"BURKINA FASO\", \n",
    "    \"MOROCCO\", \"ALGERIA\", \"COMORO ISLANDS\", \"EQUATORIAL GUINEA\", \n",
    "    \"CENTRAL AFRICAN REPUBLIC\", \"CAPE VERDE\", \"LESOTHO\", \"SWAZILAND\", \n",
    "    \"GAMBIA\", \"SIERRA LEONE\", \"GUINEA BISSAU\"]\n",
    "\n",
    "america = [\"GUATEMALA\", \"EL SALVADOR\", \"PANAMA\", \"COLOMBIA\", \n",
    "    \"ARGENTINA\", \"HAITI\", \"VENEZUELA\", \"MEXICO\", \"CUBA\", \"DOMINICAN REPUBLIC\", \n",
    "    \"BRAZIL\", \"CHILE\", \"SURINAME\", \"TRINIDAD AND TOBAGO\", \"JAMAICA\", \n",
    "    \"CANADA\", \"USA\", \"ST. KITTS, WEST INDIES\", \"ANTIGUA AND BARBUDA\", \n",
    "    \"BARBADOS\", \"BAHAMAS\", \"BELIZE\", \"DOMINICA\", \"GRENADA\", \n",
    "    \"NICARAGUA\", \"URUGUAY\", \"PARAGUAY\", \"ST. LUCIA\", \"ST. VINCENT AND THE GRENADINES\"]\n",
    "\n",
    "asia = [\"PAKISTAN\", \"VIETNAM\", \"INDONESIA\", \"AFGHANISTAN\", \n",
    "    \"IRAN\", \"BANGLADESH\", \"PHILIPPINES\", \"TAIWAN\", \"MALAYSIA\", \n",
    "    \"KAZAKHSTAN\", \"KYRGYZSTAN\", \"THAILAND\", \"TURKMENISTAN\", \"UZBEKISTAN\", \n",
    "    \"MONGOLIA\", \"SRI LANKA\", \"BHUTAN\", \"LAOS\", \"NEPAL\", \n",
    "    \"MYANMAR\", \"KAMPUCHEA\", \"BRUNEI\", \"BURMA\", \"KOREA\", \"NORTH KOREA\"]\n",
    "\n",
    "europe = [\"RUSSIA\", \"ARMENIA\", \"ALBANIA\", \"YUGOSLAVIA\", \"UNITED KINGDOM\", \n",
    "    \"BULGARIA\", \"ROMANIA\", \"HUNGARY\", \"POLAND\", \"CZECH REPUBLIC\", \n",
    "    \"SLOVAK REPUBLIC\", \"GERMANY\", \"FRANCE\", \"ITALY\", \"SPAIN\", \n",
    "    \"SWEDEN\", \"DENMARK\", \"FINLAND\", \"AUSTRIA\", \"SWITZERLAND\", \n",
    "    \"BELGIUM\", \"GREECE\", \"NETHERLANDS\", \"CROATIA\", \"SLOVENIA\", \n",
    "    \"MONACO\", \"LITHUANIA\", \"LATVIA\", \"ESTONIA\", \"ICELAND\"]\n",
    "\n",
    "# Create the regional variable and set it to 0 by default\n",
    "df['middleast'] = 0\n",
    "df['america'] = 0\n",
    "df['africa'] = 0\n",
    "df['asia'] = 0\n",
    "df['europe'] = 0\n",
    "\n",
    "# Replace with 1 for observations where nationality is in the list of selected regions\n",
    "df.loc[df['nat_name'].isin(middle_eastern_countries), 'middleast'] = 1\n",
    "df.loc[df['nat_name'].isin(america), 'america'] = 1\n",
    "df.loc[df['nat_name'].isin(africa), 'africa'] = 1\n",
    "df.loc[df['nat_name'].isin(asia), 'asia'] = 1\n",
    "df.loc[df['nat_name'].isin(europe), 'europe'] = 1\n",
    "\n",
    "#Create interaction terms\n",
    "df['middleast_dev'] = df['middleast']*df['temp6t4']\n",
    "df['america_dev'] = df['america']*df['temp6t4']\n",
    "df['africa_dev'] = df['africa']*df['temp6t4']\n",
    "df['asia_dev'] = df['asia']*df['temp6t4']\n",
    "df['europe_dev'] = df['europe']*df['temp6t4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a categorical variable for location and group locations into regions\n",
    "northeast = ['NEWARK', 'BOSTON', 'NEW YORK CITY', 'BUFFALO', 'PHILADELPHIA', \n",
    "    'NEW YORK ANNEX', 'NY DET (VARICK ST.)', 'HARTFORD', \n",
    "    '*PA DOC.', 'CLEVELAND', '*BOP  DANBURY', '*RI  DOC',\n",
    "    '*WISCONSIN DOC', '*NH  DOC', '*SUFFOLK COUNTY','*NEWARK VIDEO HEARINGS','*JESSUP'\n",
    "    '*BOP ALLENWOOD', '*NORTHERN STATE NJ DOC','YORK COUNTY DET','YORK COUNTY DET']\n",
    "\n",
    "midwest = ['CHICAGO', 'DETROIT', 'CINCINNATI', 'CLEVELAND', 'ST. LOUIS', \n",
    "    'MEMPHIS', 'KANSAS CITY', 'OMAHA', '*MI  DOC', \n",
    "    '*IL DOC - STATESVILLE', '*MO DOC', '*OHIO DOC', \n",
    "    '*INDIANA YOUTH CENTER']\n",
    "\n",
    "south = ['ARLINGTON', 'DALLAS', 'HOUSTON', 'MIAMI', 'ATLANTA', \n",
    "    'NEW ORLEANS', 'SAN ANTONIO', 'DALLAS DET', 'SAN ANTONIO DET', \n",
    "    'HOUSTON DET', 'ATLANTA DET', '*GEORGIA DOC', '*VA DOC', \n",
    "    '*DADE COUNTY FL DOC', '*BROWARD  FL DOC', 'ORLANDO', 'KROME DET',\n",
    "    'PORT ISABEL DET', 'EL PASO', 'EL PASO DET', '*TX DOC', \n",
    "    'LOUISVILLE', 'OKLAHOMA CITY', 'OKLAHOMA CITY DET', \n",
    "    'BATAVIA SPC', 'BROWARD TRANS CTR','ST. THOMAS', 'ST. CROIX', 'ROLLING PLAINS DETENTION CENTER',\n",
    "    '*BOP BIG SPRING AIRPARK','BRADENTON DET','SAN ANTONIO DET']\n",
    "\n",
    "west = ['DENVER', 'SAN DIEGO', 'LOS ANGELES', 'SAN FRANCISCO', \n",
    "    'PHOENIX', 'LAS VEGAS', 'RENO', 'SALT LAKE CITY', 'OTAY MESA', \n",
    "    'TUCSON', 'HONOLULU', 'SAN JUAN', 'SEATTLE', 'PORTLAND',\n",
    "    'SAN FRANCISCO DET', 'DENVER DET', 'SAN DIEGO DETAINED', \n",
    "    'MIRA LOMA DET', 'HONOLULU DET', '*CO DOC', '*AZ DOC',\n",
    "    '*WA DOC', '*AK DOC', 'ANCHORAGE', 'SAN PEDRO', \n",
    "    'IMPERIAL', '*NM DOC','PORTLAND DET','*MONROE WA DOC','SAN FRANCISCO ANNEX']\n",
    "\n",
    "\n",
    "# Create the regional variable and set it to 0 by default\n",
    "df['northeast'] = 0\n",
    "df['midwest'] = 0\n",
    "df['south'] = 0\n",
    "df['west'] = 0\n",
    "\n",
    "\n",
    "# Replace with 1 for observation where location is in the list of selected regions\n",
    "df.loc[df['location'].isin(northeast), 'northeast'] = 1\n",
    "df.loc[df['location'].isin(midwest), 'midwest'] = 1\n",
    "df.loc[df['location'].isin(south), 'south'] = 1\n",
    "df.loc[df['location'].isin(west), 'west'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a date categorical variable\n",
    "df['year'] = df['date'].dt.year\n",
    "\n",
    "#create dummy for year\n",
    "df['year2000'] = 0\n",
    "df['year2001'] = 0\n",
    "df['year2002'] = 0\n",
    "df['year2003'] = 0\n",
    "df['year2004'] = 0\n",
    "\n",
    "# Replace with 1 for observations in a specific year\n",
    "df.loc[df['year'] == 2000, 'year2000'] = 1\n",
    "df.loc[df['year'] == 2001, 'year2001'] = 1\n",
    "df.loc[df['year'] == 2002, 'year2002'] = 1\n",
    "df.loc[df['year'] == 2003, 'year2003'] = 1\n",
    "df.loc[df['year'] == 2004, 'year2004'] = 1\n",
    "\n",
    "\n",
    "# Interaction term for location and year\n",
    "years = [2000, 2001, 2002, 2003, 2004]\n",
    "locations = ['northeast', 'midwest', 'south', 'west']\n",
    "\n",
    "for year in years:\n",
    "    for location in locations:\n",
    "        df[f'{location}_year{year}'] = df[location] * df[f'year{year}']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy variables for the months \n",
    "\n",
    "df['month'] = df['date'].dt.month\n",
    "df = pd.get_dummies(df, columns=['month'], prefix='month', drop_first=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean the dataset\n",
    "\n",
    "#Drop asylum cases with no classification\n",
    "df = df[df['c_asy_type'].isin(['E', 'I'])]\n",
    "\n",
    "# Clean dataset by dropping any rows with NA observations\n",
    "df_final = df.dropna(axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the summary statistics for the main variables of interest\n",
    "summary_stats = df_final[['temp6t4','heat','wind6t4','res','dummy_gender']].describe() \n",
    "print(summary_stats)\n",
    "\n",
    "mean_values = df_final[['temp6t4','heat','wind6t4','res','dummy_gender']].mean()\n",
    "print(mean_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summary statistics per region to check if the observations are balanced\n",
    "\n",
    "#List of locations\n",
    "locations = ['northeast', 'midwest', 'south', 'west']\n",
    "\n",
    "for location in locations:\n",
    "    # Filter rows where the location is 1 \n",
    "    df_filtered = df_final[df_final[location] == 1]\n",
    "    # Calculate summary statistics for resolution in the filtered data\n",
    "    summary_stats = df_filtered['res'].describe()\n",
    "    # Print the summary stats for the current location\n",
    "    print(f\"Summary statistics for {location}:\")\n",
    "    print(summary_stats)\n",
    "    print(\"\\n\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create my Y variable\n",
    "Y = np.array(df_final['res'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create X variables for different specifications\n",
    "#Note: drop one category for each dummy\n",
    "\n",
    "\n",
    "#Specification 1\n",
    "selectedvariables = ['temp6t4','heat','skycover', 'co', 'co_distance', 'ozone', 'ozone_distance', 'pm', \n",
    "                     'pm_distance', 'press6t4', 'dew6t4', 'prcp6t4', 'wind6t4', \n",
    "                     'rh6t4', 'chair', 'dummy_asylum', 'dummy_gender', \n",
    "                     'middleast', 'america', 'africa', 'europe', 'northeast', 'midwest', \n",
    "                     'south', 'year2000', 'year2001', 'year2002', \n",
    "                     'year2003','middleast_dev','america_dev','africa_dev','europe_dev','month_1',\n",
    "                     'month_2','month_3','month_4','month_5','month_6','month_7','month_8',\n",
    "                     'month_9','month_10','month_11']\n",
    "\n",
    "#Specification 2\n",
    "selectedvariables_noweather = ['temp6t4','chair', 'dummy_asylum', 'dummy_gender', \n",
    "                     'middleast', 'america', 'africa', 'europe', 'northeast', 'midwest', \n",
    "                     'south', 'year2000', 'year2001', 'year2002', \n",
    "                     'year2003','middleast_dev','america_dev','africa_dev','europe_dev','month_1','month_2',\n",
    "                      'month_3','month_4','month_5','month_6','month_7','month_8',\n",
    "                    'month_9','month_10','month_11']\n",
    "\n",
    "#Additional Fixed Effects\n",
    "year_location = ['northeast_year2000', 'northeast_year2001', 'northeast_year2002', 'northeast_year2003', \n",
    "                 'northeast_year2004','midwest_year2000', 'midwest_year2001', 'midwest_year2002', \n",
    "                 'midwest_year2003', 'midwest_year2004','south_year2000', 'south_year2001', 'south_year2002', \n",
    "                 'south_year2003', 'south_year2004','west_year2000', 'west_year2001', 'west_year2002', \n",
    "                 'west_year2003']\n",
    "\n",
    "# Deviation Specification - Future Steps\n",
    "selectedvariables_deviation = ['deviation','heat','skycover', 'co', 'co_distance', 'ozone', 'ozone_distance', 'pm', \n",
    "                     'pm_distance', 'press6t4', 'dew6t4', 'prcp6t4', 'wind6t4', \n",
    "                     'rh6t4', 'chair', 'dummy_asylum', 'dummy_gender', \n",
    "                     'middleast', 'america', 'africa', 'europe', 'northeast', 'midwest', \n",
    "                     'south', 'year2000', 'year2001', 'year2002', \n",
    "                     'year2003','middleast_dev','america_dev','africa_dev','europe_dev','month_1',\n",
    "                     'month_2','month_3','month_4','month_5','month_6','month_7','month_8',\n",
    "                     'month_9','month_10','month_11']\n",
    "\n",
    "all_variables = selectedvariables + year_location\n",
    "                        \n",
    "#Create X variables with different specification\n",
    "X = df_final[selectedvariables]\n",
    "\n",
    "X_no_control = df_final[selectedvariables_noweather]\n",
    "\n",
    "X_all = df_final[all_variables]\n",
    "\n",
    "X_deviation = df_final[selectedvariables_deviation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specification 1\n",
    "model_1 = Probit(Y, X.astype(float))\n",
    "probit_model1 = model_1.fit()\n",
    "print(probit_model1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate predicted probabilities\n",
    "predicted_probs = probit_model1.predict(X.astype(float))\n",
    "\n",
    "# Calculate marginal effect for the variable of interest\n",
    "x_temp6t4 = X['temp6t4']  \n",
    "marginal_effect_temp = probit_model1.params['temp6t4'] * predicted_probs * (1 - predicted_probs)\n",
    "average_marginal_effect_temp = np.mean(marginal_effect_temp)\n",
    "\n",
    "print(f\"Average Marginal Effect: {average_marginal_effect_temp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specification 1 - Deviation - Future Steps\n",
    "\n",
    "model_1_deviation = Probit(Y, X_deviation.astype(float))\n",
    "probit_model1_deviation = model_1_deviation.fit()\n",
    "print(probit_model1_deviation.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specification 2\n",
    "\n",
    "model_2 = Probit(Y, X_no_control.astype(float))\n",
    "probit_model2 = model_2.fit()\n",
    "print(probit_model2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate predicted probabilities\n",
    "predicted_probs = probit_model2.predict(X_no_control.astype(float))\n",
    "\n",
    "# Calculate marginal effect for the variable of interest \n",
    "x_temp6t4 = X['temp6t4']  \n",
    "marginal_effect_temp = probit_model2.params['temp6t4'] * predicted_probs * (1 - predicted_probs)\n",
    "\n",
    "# Average marginal effect\n",
    "average_marginal_effect_temp = np.mean(marginal_effect_temp)\n",
    "\n",
    "print(f\"Average Marginal Effect: {average_marginal_effect_temp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specification 3 - Additional Fixed Effects - Not included in the report\n",
    "model_3 = Probit(Y, X_all.astype(float))\n",
    "probit_model3 = model_3.fit()\n",
    "print(probit_model3.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specification - Future Steps - Per region\n",
    "#Change the df_final['location'] for each region of interest\n",
    "\n",
    "df_filtered = df_final[df_final['midwest'] == 1]\n",
    "\n",
    "Y_location = np.array(df_filtered['res'])  \n",
    "\n",
    "#Remove the location fixed effects as the filtered dataset only includes one region\n",
    "selectedvariables = ['temp6t4','heat','skycover', 'co', 'co_distance', 'ozone', 'ozone_distance', 'pm', \n",
    "                     'pm_distance', 'press6t4', 'dew6t4', 'prcp6t4', 'wind6t4', \n",
    "                     'rh6t4', 'chair', 'dummy_asylum', 'dummy_gender', \n",
    "                     'middleast', 'america', 'africa', 'europe', 'year2000', 'year2001', 'year2002', \n",
    "                     'year2003','middleast_dev','america_dev','africa_dev','europe_dev','month_1',\n",
    "                     'month_2','month_3','month_4','month_5','month_6','month_7','month_8',\n",
    "                     'month_9','month_10','month_11']\n",
    "\n",
    "X_location = df_filtered[selectedvariables]  \n",
    "\n",
    "#Run specification 1 with filtered dataset\n",
    "model_4 = Probit(Y_location, X_location.astype(float))\n",
    "probit_model4 = model_4.fit()\n",
    "print(probit_model4.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame per months (winter vs non. winter months)\n",
    "\n",
    "df_final['date'] = pd.to_datetime(df_final['date'])\n",
    "\n",
    "#Filter the dataset to include only months we want (winter vs non winter months)\n",
    "df_filter_nowinter = df_final[df_final['date'].dt.month.isin([3, 4, 5, 6, 7, 8, 9, 10, 11])]\n",
    "df_filter_winter = df_final[df_final['date'].dt.month.isin([1,2,12])]\n",
    "\n",
    "# Define selected variables for no winter, drop winter months dummies\n",
    "selectedvariables_nowinter = ['skycover', 'co', 'co_distance', 'ozone', 'ozone_distance', 'pm', \n",
    "                     'pm_distance', 'temp6t4','deviation', 'press6t4', 'dew6t4', 'prcp6t4', 'wind6t4', \n",
    "                     'rh6t4', 'heat', 'chair', 'dummy_asylum', 'dummy_gender', \n",
    "                     'middleast', 'america', 'africa', 'europe', 'northeast', 'midwest', \n",
    "                     'south', 'year2000', 'year2001', 'year2002', \n",
    "                     'year2003','middleast_dev','america_dev','africa_dev','europe_dev','month_3',\n",
    "                     'month_4','month_5','month_6','month_7','month_8',\n",
    "                     'month_9','month_10']\n",
    "\n",
    "# Define selected variables for winter, drop non-winter months dummies\n",
    "selectedvariables_winter = ['skycover', 'co', 'co_distance', 'ozone', 'ozone_distance', 'pm', \n",
    "                     'pm_distance', 'temp6t4','deviation', 'press6t4', 'dew6t4', 'prcp6t4', 'wind6t4', \n",
    "                     'rh6t4', 'heat', 'chair', 'dummy_asylum', 'dummy_gender', \n",
    "                     'middleast', 'america', 'africa', 'europe', 'northeast', 'midwest', \n",
    "                     'south', 'year2000', 'year2001', 'year2002', \n",
    "                     'year2003','middleast_dev','america_dev','africa_dev','europe_dev','month_1','month_2']\n",
    "\n",
    "\n",
    "#Create X and Y variables\n",
    "Y_nowinter = np.array(df_filter_nowinter['res'])  \n",
    "X_nowinter = df_filter_nowinter[selectedvariables_nowinter]  \n",
    "\n",
    "Y_winter = np.array(df_filter_winter['res'])  \n",
    "X_winter = df_filter_winter[selectedvariables_winter]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model with dataframe filtered with no winter months - Future Steps\n",
    "\n",
    "model_5 = Probit(Y_nowinter, X_nowinter.astype(float))\n",
    "probit_model5 = model_5.fit()\n",
    "print(probit_model5.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model with dataframe filtered with only winter months - Future Steps\n",
    "\n",
    "model_6 = Probit(Y_winter, X_winter.astype(float))\n",
    "probit_model6 = model_6.fit()\n",
    "print(probit_model6.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most interesting finding is that temperature has a positive coefficient in our logit model (opposite to the original paper) and almost 0 in the Ridge model. Meanwhile, in the Lasso model temperature has a negative coefficient in line with the original paper. Perhaps when more variables are included the effect of temperature is diminished. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Ridge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run a Ridge Model\n",
    "#Code in this section based on the notebook: ISLP-Ch06_varselect_lab.ipynb\n",
    "\n",
    "coefficients = []\n",
    "#Calculate lambda from 10^8 to 10^-2\n",
    "lambdas = 10**np.linspace(8, -2, 100) / Y.std()\n",
    "scaler = StandardScaler(with_mean=True,  with_std=True)\n",
    "for lam in lambdas:\n",
    "    ridge = SGDClassifier(loss='log_loss', penalty='l2',alpha=lam)\n",
    "    # Create a pipeline with scaling and the classifier\n",
    "    pipe = Pipeline(steps=[('scaler', scaler), ('ridge', ridge)])\n",
    "    # Fit the pipeline to the data\n",
    "    pipe.fit(X, Y)\n",
    "    # Store the coefficients \n",
    "    coefficients.append(pipe.named_steps['ridge'].coef_.flatten())\n",
    "\n",
    "#solution containing all our coefficients    \n",
    "soln_array = np.array(coefficients)\n",
    "\n",
    "# Create a DataFrame with the solution path, for easy  transposing soln_array so features are in columns\n",
    "soln_path = pd.DataFrame(soln_array, columns=X.columns, index=-np.log(lambdas))\n",
    "# Name the index to indicate it's the negative log of lambda\n",
    "soln_path.index.name = 'negative log(lambda)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the graph\n",
    "path_fig, ax = subplots(figsize=(8,8))\n",
    "soln_path.plot(ax=ax, legend=False)\n",
    "ax.set_xlabel('$-\\log(\\lambda)$', fontsize=20)\n",
    "ax.set_ylabel('Standardized coefficients', fontsize=20)\n",
    "ax.legend(loc='upper left');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize K-Fold cross-validation strategy\n",
    "kfold = KFold(n_splits=5)\n",
    "\n",
    "param_grid = {'ridge__alpha': lambdas}\n",
    "\n",
    "# Use GridSearchCV to find the best alpha using cross-validation\n",
    "#Since we have a categorical variable the scoring is accuracy (where it defines how accurate we predict y)\n",
    "grid_search = GridSearchCV(pipe, param_grid, cv=kfold, scoring='accuracy', return_train_score=True)\n",
    "\n",
    "# Fit the model using cross-validation to find the best alpha\n",
    "grid_search.fit(X, Y)\n",
    "\n",
    "# Find optimal lambda\n",
    "tuned_ridge = grid_search.best_estimator_.named_steps['ridge']\n",
    "\n",
    "# Retrieve the mean and standard deviation of cross-validation scores\n",
    "mean_scores = grid_search.cv_results_['mean_test_score']\n",
    "std_scores = grid_search.cv_results_['std_test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the Cross-Validated Accuracy and Error Bars\n",
    "ridgeCV_fig, ax = subplots(figsize=(8, 8))\n",
    "ax.errorbar(-np.log(lambdas), mean_scores, yerr=std_scores / np.sqrt(kfold.get_n_splits()), fmt='o')\n",
    "ax.axvline(-np.log(grid_search.best_params_['ridge__alpha']), c='k', ls='--')\n",
    "ax.set_xlabel('$-\\log(\\\\lambda)$', fontsize=20)\n",
    "ax.set_ylabel('Cross-validated Accuracy', fontsize=20)\n",
    "ax.set_title('Cross-Validation Accuracy with Error Bars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = tuned_ridge.coef_.flatten()  \n",
    "variable_names = X.columns  # Get the names of the features\n",
    "\n",
    "# Create a dictionary mapping variable names to their coefficients\n",
    "coef_mapping = {variable: coef for variable, coef in zip(variable_names, coefficients)}\n",
    "\n",
    "# Convert the dictionary to a DataFrame for better visualization\n",
    "coef_df = pd.DataFrame(list(coef_mapping.items()), columns=['Variable', 'Coefficient'])\n",
    "print(coef_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code in this section based on the notebook: ISLP-Ch06_varselect_lab.ipynb\n",
    "\n",
    "#Running Lasso\n",
    "coefficients_l = []\n",
    "for lam in lambdas:\n",
    "    lasso = SGDClassifier(loss='log_loss', penalty='l1',alpha=lam)\n",
    "    # Create a pipeline with scaling and the classifier\n",
    "    pipe_l = Pipeline(steps=[('scaler', scaler), ('lasso', lasso)])\n",
    "    # Fit the pipeline to the data\n",
    "    pipe_l.fit(X, Y)\n",
    "    # Store the coefficients \n",
    "    coefficients_l.append(pipe_l.named_steps['lasso'].coef_.flatten())\n",
    "\n",
    "soln_array_l = np.array(coefficients_l)\n",
    "# Create a DataFrame with the solution path, transposing soln_array so features are in columns\n",
    "soln_path_l = pd.DataFrame(soln_array_l, columns=X.columns, index=-np.log(lambdas))\n",
    "# Name the index to indicate it's the negative log of lambda\n",
    "soln_path_l.index.name = 'negative log(lambda)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_fig_l, ax = subplots(figsize=(8,8))\n",
    "soln_path_l.plot(ax=ax, legend=False)\n",
    "ax.set_xlabel('$-\\log(\\lambda)$', fontsize=20)\n",
    "ax.set_ylabel('Standardized coefficients', fontsize=20)\n",
    "ax.legend(loc='upper left');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=5)\n",
    "\n",
    "# Define a grid of alpha values to search over\n",
    "param_grid_l = {'lasso__alpha': lambdas}\n",
    "\n",
    "# Use GridSearchCV to find the best alpha using cross-validation\n",
    "# Since we have a categorical variable the scoring is not MSE but accuracy\n",
    "grid_search_l = GridSearchCV(pipe_l, param_grid_l, cv=kfold, scoring='accuracy', return_train_score=True)\n",
    "\n",
    "# Fit the model using cross-validation to find the best alpha\n",
    "grid_search_l.fit(X, Y)\n",
    "\n",
    "# Get the tuned LASSO model (SGDClassifier)\n",
    "tuned_lasso = grid_search_l.best_estimator_.named_steps['lasso']\n",
    "\n",
    "# Retrieve the mean and standard deviation of cross-validation scores\n",
    "mean_scores_l = grid_search_l.cv_results_['mean_test_score']\n",
    "std_scores_l = grid_search_l.cv_results_['std_test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the Cross-Validated Accuracy and Error Bars\n",
    "lassoCV_fig, ax = subplots(figsize=(8, 8))\n",
    "ax.errorbar(-np.log(lambdas), mean_scores_l, yerr=std_scores_l / np.sqrt(kfold.get_n_splits()), fmt='o')\n",
    "ax.axvline(-np.log(grid_search_l.best_params_['lasso__alpha']), c='k', ls='--')\n",
    "ax.set_xlabel('$-\\log(\\\\lambda)$', fontsize=20)\n",
    "ax.set_ylabel('Cross-validated Accuracy', fontsize=20)\n",
    "ax.set_title('Cross-Validation Accuracy with Error Bars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients_l = tuned_lasso.coef_.flatten()  \n",
    "variable_names = X.columns  # Get the names of the features\n",
    "\n",
    "# Create a dictionary mapping variable names to their coefficients\n",
    "coef_mapping_l = {variable: coef for variable, coef in zip(variable_names, coefficients_l)}\n",
    "\n",
    "# Convert the dictionary to a DataFrame for better visualization\n",
    "coef_df = pd.DataFrame(list(coef_mapping_l.items()), columns=['Variable', 'Coefficient'])\n",
    "print(coef_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresion Trees "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code in this section based on the notebook: ISLP-TreeModels.ipynb\n",
    "\n",
    "clf = DTC(criterion='entropy', \n",
    "          max_depth = 3,\n",
    "          random_state=0)   \n",
    "\n",
    "clf.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify columns\n",
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename columns for better labelling of trees\n",
    "X_detailed = ['Average temperature', 'Heat index', 'Sky coverage', 'Carbon monoxide levels', \n",
    "              'Distance CO source', 'Ozone levels', 'Distance ozone source', 'PM levels', \n",
    "              'Distance PM source', 'Atmospheric pressure', 'Dew point temperature', 'Precipitation', \n",
    "              'Wind speed', 'Relative humidity', 'Judge identifier', 'Asylum application', \n",
    "              'Gender', 'Middle Eastern', 'American', 'African', 'European', 'Northeast', 'Midwest', 'South', \n",
    "              '2000','2001', '2002', '2003', 'Interaction of temperature and Middle Eastern',\n",
    "              'Interaction of temperature and American', 'Interaction of temperature and African', \n",
    "              'Interaction of temperature and European', 'January', 'February', 'March', 'April', 'May', 'June', \n",
    "              'July', 'August', 'September', 'October', 'November']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = X_detailed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(Y, clf.predict(X)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = subplots(figsize=(12,12))[1]\n",
    "plot_tree(clf,\n",
    "          feature_names=feature_names,\n",
    "          ax=ax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross validation \n",
    "validation = skm.ShuffleSplit(n_splits=1,\n",
    "                              test_size=200,\n",
    "                              random_state=0)\n",
    "results = skm.cross_validate(clf,\n",
    "                             X,\n",
    "                             Y,\n",
    "                             cv=validation)\n",
    "results['test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split dataset\n",
    "(X_train,\n",
    " X_test,\n",
    " Y_train,\n",
    " Y_test) = skm.train_test_split(X,\n",
    "                                   Y,\n",
    "                                   test_size=0.5,\n",
    "                                   random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DTC(criterion='entropy', random_state=0)\n",
    "clf.fit(X_train, Y_train)\n",
    "accuracy_score(Y_test, clf.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccp_path = clf.cost_complexity_pruning_path(X_train, Y_train)\n",
    "kfold = skm.KFold(5,\n",
    "                  random_state=1,\n",
    "                  shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code in this section based on the notebook: ISLP-TreeModels.ipynb\n",
    "\n",
    "bag_temperature = RFC(max_features=X_train.shape[1], random_state=0)\n",
    "bag_temperature.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_temperature = RFC(max_features=X_train.shape[1],\n",
    "                n_estimators=500,#how many trees you are running\n",
    "                random_state=0).fit(X_train, Y_train)\n",
    "y_hat_bag = bag_temperature.predict(X_test)\n",
    "accuracy_bagging = accuracy_score(Y_test, y_hat_bag)\n",
    "accuracy_bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_imp_bag = pd.DataFrame(\n",
    "    {'importance':bag_temperature.feature_importances_},\n",
    "    index=feature_names)\n",
    "feature_imp_bag.sort_values(by='importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_imp_bag = pd.DataFrame(\n",
    "    {'importance': bag_temperature.feature_importances_},\n",
    "    index=feature_names\n",
    ")\n",
    "\n",
    "# Sort the feature importances\n",
    "feature_imp_bag = feature_imp_bag.sort_values(by='importance', ascending=False)\n",
    "\n",
    "# Plot the feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_imp_bag.index, feature_imp_bag['importance'], color='skyblue')\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Feature Importances (Bagged Model)')\n",
    "plt.gca().invert_yaxis()  # To display the most important feature at the top\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code in this section based on the notebook: ISLP-TreeModels.ipynb\n",
    "\n",
    "RF_temperature = RFC(max_features=6,\n",
    "               random_state=0).fit(X_train, Y_train)\n",
    "y_hat_RF = RF_temperature.predict(X_test)\n",
    "accuracy_RF = accuracy_score(Y_test, y_hat_RF)\n",
    "accuracy_RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_imp = pd.DataFrame(\n",
    "    {'importance':RF_temperature.feature_importances_},\n",
    "    index=feature_names)\n",
    "feature_imp.sort_values(by='importance', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code in this section based on the notebook: ISLP-TreeModels.ipynb\n",
    "\n",
    "boost_temperature = GBC(n_estimators=500,\n",
    "                   learning_rate=0.001,\n",
    "                    max_depth = 3,\n",
    "                   random_state=0)\n",
    "boost_temperature.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boost_temperature = GBC(n_estimators=500,\n",
    "                   learning_rate=0.001,\n",
    "                    max_depth = 3,\n",
    "                   random_state=0)\n",
    "boost_temperature.fit(X_train,Y_train)\n",
    "y_hat_boost = boost_temperature.predict(X_test);\n",
    "accuracy_boosting = accuracy_score(Y_test, y_hat_boost)\n",
    "accuracy_boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_imp = pd.DataFrame(\n",
    "    {'importance':boost_temperature.feature_importances_},\n",
    "    index=feature_names)\n",
    "feature_imp.sort_values(by='importance', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directed Acyclic Graph (DAG) and Causal Relationship "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code in this section based on the notebook: CIDP-Chapter_04\n",
    "\n",
    "# Define the graph\n",
    "sample_gml = \"\"\"graph [\n",
    "directed 1\n",
    "\n",
    "node [\n",
    "    id 1\n",
    "    label \"Cognitive\"\n",
    "    ]\n",
    "    \n",
    "node [\n",
    "    id 2\n",
    "    label \"Weather\"\n",
    "    ]\n",
    "\n",
    "node [\n",
    "    id 4\n",
    "    label \"Resolution\"\n",
    "    ]\n",
    "    \n",
    "node [\n",
    "    id 5\n",
    "    label \"Location\"\n",
    "    ]\n",
    "\n",
    "node [\n",
    "    id 6\n",
    "    label \"Judge\"\n",
    "    ]\n",
    "\n",
    "node [\n",
    "    id 7\n",
    "    label \"Nationality\"\n",
    "    ]\n",
    "    \n",
    "edge [\n",
    "    source 2\n",
    "    target 1\n",
    "    ]\n",
    "\n",
    "edge [\n",
    "    source 5\n",
    "    target 4\n",
    "    ]\n",
    "\n",
    "edge [\n",
    "    source 6\n",
    "    target 4\n",
    "    ]\n",
    "\n",
    "edge [\n",
    "    source 1\n",
    "    target 6\n",
    "    ]\n",
    "edge [\n",
    "    source 7\n",
    "    target 4\n",
    "    ]\n",
    "edge [\n",
    "    source 5\n",
    "    target 2\n",
    "    ]\n",
    "]\n",
    "    \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the graph\n",
    "graph = nx.parse_gml(sample_gml)\n",
    "\n",
    "# Plot\n",
    "nx.draw(\n",
    "    G=graph, \n",
    "    with_labels=True,\n",
    "    node_size=2500,\n",
    "    node_color=COLORS[0],\n",
    "    font_color='black',\n",
    "    font_size = 8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the graph\n",
    "sample_gml2 = \"\"\"graph [\n",
    "directed 1\n",
    "\n",
    "node [\n",
    "    id 1\n",
    "    label \"Cognitive\"\n",
    "    ]\n",
    "    \n",
    "node [\n",
    "    id 2\n",
    "    label \"Weather\"\n",
    "    ]\n",
    "\n",
    "node [\n",
    "    id 4\n",
    "    label \"Resolution\"\n",
    "    ]\n",
    "    \n",
    "node [\n",
    "    id 5\n",
    "    label \"Location\"\n",
    "    ]\n",
    "\n",
    "node [\n",
    "    id 6\n",
    "    label \"Judge\"\n",
    "    ]\n",
    "\n",
    "node [\n",
    "    id 7\n",
    "    label \"Nationality\"\n",
    "    ]\n",
    "    \n",
    "edge [\n",
    "    source 2\n",
    "    target 1\n",
    "    ]\n",
    "\n",
    "edge [\n",
    "    source 5\n",
    "    target 4\n",
    "    ]\n",
    "\n",
    "edge [\n",
    "    source 6\n",
    "    target 4\n",
    "    ]\n",
    "\n",
    "edge [\n",
    "    source 1\n",
    "    target 6\n",
    "    ]\n",
    "edge [\n",
    "    source 7\n",
    "    target 1\n",
    "    ]\n",
    "edge [\n",
    "    source 5\n",
    "    target 2\n",
    "    ]\n",
    "]\n",
    "    \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the graph\n",
    "graph = nx.parse_gml(sample_gml2)\n",
    "\n",
    "# Plot\n",
    "nx.draw(\n",
    "    G=graph, \n",
    "    with_labels=True,\n",
    "    node_size=2500,\n",
    "    node_color=COLORS[0],\n",
    "    font_color='black',\n",
    "    font_size = 8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the graph\n",
    "sample_gml3 = \"\"\"graph [\n",
    "directed 1\n",
    "\n",
    "node [\n",
    "    id 1\n",
    "    label \"Cognitive\"\n",
    "    ]\n",
    "    \n",
    "node [\n",
    "    id 2\n",
    "    label \"Weather\"\n",
    "    ]\n",
    "\n",
    "node [\n",
    "    id 4\n",
    "    label \"Resolution\"\n",
    "    ]\n",
    "    \n",
    "node [\n",
    "    id 5\n",
    "    label \"Location\"\n",
    "    ]\n",
    "\n",
    "node [\n",
    "    id 6\n",
    "    label \"Judge\"\n",
    "    ]\n",
    "\n",
    "node [\n",
    "    id 7\n",
    "    label \"Nationality\"\n",
    "    ]\n",
    "    \n",
    "edge [\n",
    "    source 2\n",
    "    target 1\n",
    "    ]\n",
    "\n",
    "edge [\n",
    "    source 6\n",
    "    target 4\n",
    "    ]\n",
    "\n",
    "edge [\n",
    "    source 1\n",
    "    target 6\n",
    "    ]\n",
    "edge [\n",
    "    source 7\n",
    "    target 1\n",
    "    ]\n",
    "edge [\n",
    "    source 5\n",
    "    target 2\n",
    "    ]\n",
    "edge [\n",
    "    source 5\n",
    "    target 6\n",
    "    ]\n",
    "]\n",
    "    \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the graph\n",
    "graph = nx.parse_gml(sample_gml3)\n",
    "\n",
    "# Plot\n",
    "nx.draw(\n",
    "    G=graph, \n",
    "    with_labels=True,\n",
    "    node_size=2500,\n",
    "    node_color=COLORS[0],\n",
    "    font_color='black',\n",
    "    font_size = 8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the graph\n",
    "gml_final = \"\"\"graph [\n",
    "directed 1\n",
    "    \n",
    "node [\n",
    "    id 1\n",
    "    label \"midwest\"\n",
    "    ]\n",
    "\n",
    "node [\n",
    "    id 2\n",
    "    label \"deviation\"\n",
    "    ]\n",
    "\n",
    "node [\n",
    "    id 4\n",
    "    label \"res\"\n",
    "    ]\n",
    "    \n",
    "node [\n",
    "    id 5\n",
    "    label \"northeast\"\n",
    "    ]\n",
    "\n",
    "node [\n",
    "    id 6\n",
    "    label \"chair\"\n",
    "    ]\n",
    "\n",
    "node [\n",
    "    id 7\n",
    "    label \"america\"\n",
    "    ]\n",
    "node [\n",
    "    id 8\n",
    "    label \"south\"\n",
    "    ]\n",
    "node [\n",
    "    id 9\n",
    "    label \"west\"\n",
    "    ]\n",
    "node [\n",
    "    id 11\n",
    "    label \"cognitive\"\n",
    "    ]\n",
    "    \n",
    "edge [\n",
    "    source 2\n",
    "    target 11\n",
    "    ]\n",
    "\n",
    "edge [\n",
    "    source 6\n",
    "    target 4\n",
    "    ]\n",
    "\n",
    "edge [\n",
    "    source 7\n",
    "    target 11\n",
    "    ]\n",
    "edge [\n",
    "    source 5\n",
    "    target 2\n",
    "    ]\n",
    "edge [\n",
    "    source 1\n",
    "    target 2\n",
    "    ]\n",
    "edge [\n",
    "    source 8\n",
    "    target 2\n",
    "    ]\n",
    "edge [\n",
    "    source 9\n",
    "    target 2\n",
    "    ]\n",
    "edge [\n",
    "    source 5\n",
    "    target 11\n",
    "    ]\n",
    "edge [\n",
    "    source 1\n",
    "    target 11\n",
    "    ]\n",
    "edge [\n",
    "    source 8\n",
    "    target 11\n",
    "    ]\n",
    "edge [\n",
    "    source 9\n",
    "    target 11\n",
    "    ]\n",
    "edge [ \n",
    "    source 11\n",
    "    target 6\n",
    "    ]\n",
    "]\n",
    "\n",
    "    \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the graph\n",
    "graph = nx.parse_gml(gml_final)\n",
    "\n",
    "# Plot\n",
    "nx.draw(\n",
    "    G=graph, \n",
    "    with_labels=True,\n",
    "    node_size=2500,\n",
    "    node_color=COLORS[0],\n",
    "    font_color='black',\n",
    "    font_size = 8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test DAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code in this section based on the notebook:  CIDP-Chapter_07\n",
    "\n",
    "#Model the problem\n",
    "model = CausalModel(\n",
    "data=df_final,\n",
    "treatment=['deviation'],\n",
    "outcome=\"res\",\n",
    "graph=gml_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify the estimand\n",
    "estimand = model.identify_effect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(estimand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtain estimates\n",
    "estimate = model.estimate_effect(\n",
    "identified_estimand=estimand,\n",
    "method_name=\"backdoor.linear_regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(estimate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform refutation test\n",
    "#Refutation test on whether estimate is influenced by unobserved confounders = random_common_cause \n",
    "refute_subset = model.refute_estimate(\n",
    "estimand=estimand,\n",
    "estimate=estimate,\n",
    "method_name=\"random_common_cause\",\n",
    "subset_fraction=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(refute_subset)\n",
    "#High p-value suuggests that the random common cause does not have a meaningful impact on the relationship between \n",
    "#temperature and the outcome, providing confidence in the stability of findings."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
