{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2 - Meta Learners and Doubly Robust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data from: Heyes, Anthony, and Soodeh Saberian. 2019. \"Temperature and Decisions: Evidence from 207,000 Court Cases.\" American Economic Journal: Applied Economics, 11 (2): 238â€“65.\n",
    "\n",
    "Notebooks used troughout the code: \n",
    "- CIBT-11-Propensity-Score\n",
    "- CIBT-21 Meta-Learners\n",
    "- CIBT-12-Doubly-Robust-Estimation\n",
    "- Doubly Robust Learner and Interpretability-econml notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LassoCV\n",
    "!pip install econml\n",
    "from econml.dr import LinearDRLearner\n",
    "from joblib import Parallel, delayed \n",
    "import shap\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from econml.dml import CausalForestDML\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Description "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_stata('matched_corrected.dta')\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dummy for asylum\n",
    "df['dummy_asylum'] = df['c_asy_type'].apply(lambda x: 1 if x == 'E' else 0)\n",
    "#Create a dummy for gender\n",
    "df['dummy_gender'] = df['gender'].apply(lambda x: 1 if x == 'female' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As outlined in the correction article drop the observation for China\n",
    "df = df[df['nat_name'] != 'CHINA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique values to identify variables for the dummy variables\n",
    "unique__names = df['nat_name'].unique()\n",
    "locations = df['location'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a categorical variable for nationatility\n",
    "\n",
    "middle_eastern_countries = [\"BAHRAIN\", \"CYPRUS\", \"EGYPT\", \"IRAN\", \"IRAQ\", \"ISRAEL\", \"JORDAN\", \n",
    "    \"KUWAIT\", \"LEBANON\", \"OMAN\", \"PALESTINE\", \"QATAR\", \"SAUDI ARABIA\", \n",
    "    \"SYRIA\", \"TURKEY\", \"UNITED ARAB EMIRATES\", \"YEMEN\"]\n",
    "\n",
    "africa = [\"ERITREA\", \"RWANDA\", \"SOMALIA\", \"SUDAN\", \"CONGO\", \"ETHIOPIA\", \"LIBYA\", \n",
    "    \"MALI\", \"ANGOLA\", \"BURUNDI\", \"TANZANIA\", \"NIGERIA\", \"GABON\", \"GHANA\", \n",
    "    \"SENEGAL\", \"CHAD\", \"DJIBOUTI\", \"CAMEROON\", \"UGANDA\", \"KENYA\", \n",
    "    \"ZAMBIA\", \"MAURITANIA\", \"SOUTH AFRICA\", \"GUINEA\", \"BURKINA FASO\", \n",
    "    \"MOROCCO\", \"ALGERIA\", \"COMORO ISLANDS\", \"EQUATORIAL GUINEA\", \n",
    "    \"CENTRAL AFRICAN REPUBLIC\", \"CAPE VERDE\", \"LESOTHO\", \"SWAZILAND\", \n",
    "    \"GAMBIA\", \"SIERRA LEONE\", \"GUINEA BISSAU\"]\n",
    "\n",
    "america = [\"GUATEMALA\", \"EL SALVADOR\", \"PANAMA\", \"COLOMBIA\", \n",
    "    \"ARGENTINA\", \"HAITI\", \"VENEZUELA\", \"MEXICO\", \"CUBA\", \"DOMINICAN REPUBLIC\", \n",
    "    \"BRAZIL\", \"CHILE\", \"SURINAME\", \"TRINIDAD AND TOBAGO\", \"JAMAICA\", \n",
    "    \"CANADA\", \"USA\", \"ST. KITTS, WEST INDIES\", \"ANTIGUA AND BARBUDA\", \n",
    "    \"BARBADOS\", \"BAHAMAS\", \"BELIZE\", \"DOMINICA\", \"GRENADA\", \n",
    "    \"NICARAGUA\", \"URUGUAY\", \"PARAGUAY\", \"ST. LUCIA\", \"ST. VINCENT AND THE GRENADINES\"]\n",
    "\n",
    "asia = [\"PAKISTAN\", \"VIETNAM\", \"INDONESIA\", \"AFGHANISTAN\", \n",
    "    \"IRAN\", \"BANGLADESH\", \"PHILIPPINES\", \"TAIWAN\", \"MALAYSIA\", \n",
    "    \"KAZAKHSTAN\", \"KYRGYZSTAN\", \"THAILAND\", \"TURKMENISTAN\", \"UZBEKISTAN\", \n",
    "    \"MONGOLIA\", \"SRI LANKA\", \"BHUTAN\", \"LAOS\", \"NEPAL\", \n",
    "    \"MYANMAR\", \"KAMPUCHEA\", \"BRUNEI\", \"BURMA\", \"KOREA\", \"NORTH KOREA\"]\n",
    "\n",
    "europe = [\"RUSSIA\", \"ARMENIA\", \"ALBANIA\", \"YUGOSLAVIA\", \"UNITED KINGDOM\", \n",
    "    \"BULGARIA\", \"ROMANIA\", \"HUNGARY\", \"POLAND\", \"CZECH REPUBLIC\", \n",
    "    \"SLOVAK REPUBLIC\", \"GERMANY\", \"FRANCE\", \"ITALY\", \"SPAIN\", \n",
    "    \"SWEDEN\", \"DENMARK\", \"FINLAND\", \"AUSTRIA\", \"SWITZERLAND\", \n",
    "    \"BELGIUM\", \"GREECE\", \"NETHERLANDS\", \"CROATIA\", \"SLOVENIA\", \n",
    "    \"MONACO\", \"LITHUANIA\", \"LATVIA\", \"ESTONIA\", \"ICELAND\"]\n",
    "\n",
    "df['middleast'] = 0\n",
    "df['america'] = 0\n",
    "df['africa'] = 0\n",
    "df['asia'] = 0\n",
    "df['europe'] = 0\n",
    "\n",
    "df.loc[df['nat_name'].isin(middle_eastern_countries), 'middleast'] = 1\n",
    "df.loc[df['nat_name'].isin(america), 'america'] = 1\n",
    "df.loc[df['nat_name'].isin(africa), 'africa'] = 1\n",
    "df.loc[df['nat_name'].isin(asia), 'asia'] = 1\n",
    "df.loc[df['nat_name'].isin(europe), 'europe'] = 1\n",
    "\n",
    "#Create interaction terms\n",
    "df['middleast_dev'] = df['middleast']*df['temp6t4']\n",
    "df['america_dev'] = df['america']*df['temp6t4']\n",
    "df['africa_dev'] = df['africa']*df['temp6t4']\n",
    "df['asia_dev'] = df['asia']*df['temp6t4']\n",
    "df['europe_dev'] = df['europe']*df['temp6t4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a categorical variable for location and group locations into regions\n",
    "northeast = ['NEWARK', 'BOSTON', 'NEW YORK CITY', 'BUFFALO', 'PHILADELPHIA', \n",
    "    'NEW YORK ANNEX', 'NY DET (VARICK ST.)', 'HARTFORD', \n",
    "    '*PA DOC.', 'CLEVELAND', '*BOP  DANBURY', '*RI  DOC',\n",
    "    '*WISCONSIN DOC', '*NH  DOC', '*SUFFOLK COUNTY','*NEWARK VIDEO HEARINGS','*JESSUP'\n",
    "    '*BOP ALLENWOOD', '*NORTHERN STATE NJ DOC','YORK COUNTY DET','YORK COUNTY DET']\n",
    "\n",
    "midwest = ['CHICAGO', 'DETROIT', 'CINCINNATI', 'CLEVELAND', 'ST. LOUIS', \n",
    "    'MEMPHIS', 'KANSAS CITY', 'OMAHA', '*MI  DOC', \n",
    "    '*IL DOC - STATESVILLE', '*MO DOC', '*OHIO DOC', \n",
    "    '*INDIANA YOUTH CENTER']\n",
    "\n",
    "south = ['ARLINGTON', 'DALLAS', 'HOUSTON', 'MIAMI', 'ATLANTA', \n",
    "    'NEW ORLEANS', 'SAN ANTONIO', 'DALLAS DET', 'SAN ANTONIO DET', \n",
    "    'HOUSTON DET', 'ATLANTA DET', '*GEORGIA DOC', '*VA DOC', \n",
    "    '*DADE COUNTY FL DOC', '*BROWARD  FL DOC', 'ORLANDO', 'KROME DET',\n",
    "    'PORT ISABEL DET', 'EL PASO', 'EL PASO DET', '*TX DOC', \n",
    "    'LOUISVILLE', 'OKLAHOMA CITY', 'OKLAHOMA CITY DET', \n",
    "    'BATAVIA SPC', 'BROWARD TRANS CTR','ST. THOMAS', 'ST. CROIX', 'ROLLING PLAINS DETENTION CENTER',\n",
    "    '*BOP BIG SPRING AIRPARK','BRADENTON DET','SAN ANTONIO DET']\n",
    "\n",
    "west = ['DENVER', 'SAN DIEGO', 'LOS ANGELES', 'SAN FRANCISCO', \n",
    "    'PHOENIX', 'LAS VEGAS', 'RENO', 'SALT LAKE CITY', 'OTAY MESA', \n",
    "    'TUCSON', 'HONOLULU', 'SAN JUAN', 'SEATTLE', 'PORTLAND',\n",
    "    'SAN FRANCISCO DET', 'DENVER DET', 'SAN DIEGO DETAINED', \n",
    "    'MIRA LOMA DET', 'HONOLULU DET', '*CO DOC', '*AZ DOC',\n",
    "    '*WA DOC', '*AK DOC', 'ANCHORAGE', 'SAN PEDRO', \n",
    "    'IMPERIAL', '*NM DOC','PORTLAND DET','*MONROE WA DOC','SAN FRANCISCO ANNEX']\n",
    "\n",
    "df['northeast'] = 0\n",
    "df['midwest'] = 0\n",
    "df['south'] = 0\n",
    "df['west'] = 0\n",
    "\n",
    "df.loc[df['location'].isin(northeast), 'northeast'] = 1\n",
    "df.loc[df['location'].isin(midwest), 'midwest'] = 1\n",
    "df.loc[df['location'].isin(south), 'south'] = 1\n",
    "df.loc[df['location'].isin(west), 'west'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy variables for the months \n",
    "\n",
    "df['month'] = df['date'].dt.month\n",
    "df = pd.get_dummies(df, columns=['month'], prefix='month', drop_first=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a date categorical variable\n",
    "df['year'] = df['date'].dt.year\n",
    "\n",
    "df['year2000'] = 0\n",
    "df['year2001'] = 0\n",
    "df['year2002'] = 0\n",
    "df['year2003'] = 0\n",
    "df['year2004'] = 0\n",
    "\n",
    "df.loc[df['year'] == 2000, 'year2000'] = 1\n",
    "df.loc[df['year'] == 2001, 'year2001'] = 1\n",
    "df.loc[df['year'] == 2002, 'year2002'] = 1\n",
    "df.loc[df['year'] == 2003, 'year2003'] = 1\n",
    "df.loc[df['year'] == 2004, 'year2004'] = 1\n",
    "\n",
    "# Interaction term for location and year\n",
    "years = [2000, 2001, 2002, 2003, 2004]\n",
    "locations = ['northeast', 'midwest', 'south', 'west']\n",
    "\n",
    "for year in years:\n",
    "    for location in locations:\n",
    "        df[f'{location}_year{year}'] = df[location] * df[f'year{year}']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean the dataset\n",
    "\n",
    "#Drop asylum cases with no classification\n",
    "df = df[df['c_asy_type'].isin(['E', 'I'])]\n",
    "\n",
    "# Clean dataset by dropping any rows with NA observations\n",
    "df_final = df.dropna(axis=0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IPW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code in this section based on the notebook: CIBT-11-Propensity-Score\n",
    "\n",
    "#Changed for deviation, being the treatment variable because if we had only temperature \n",
    "#we might have that specific regions such as Texas is always treated etc.\n",
    "df_final['T_binary'] = (df_final['deviation'] > 0.000095).astype(int)\n",
    "print(df_final['T_binary'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 'T_binary'\n",
    "Y = 'res'\n",
    "X = ['chair', 'dummy_asylum', 'dummy_gender', \n",
    "                     'middleast', 'america', 'africa', 'europe', 'northeast', 'midwest', \n",
    "                     'south', 'year2000', 'year2001', 'year2002', \n",
    "                     'year2003','month_1',\n",
    "                     'month_2','month_3','month_4','month_5','month_6','month_7','month_8',\n",
    "                     'month_9','month_10','month_11']\n",
    "\n",
    "ps_model = LogisticRegression(C=1e6).fit(df_final[X], df_final[T])\n",
    "\n",
    "data_ps = df_final.assign(propensity_score=ps_model.predict_proba(df_final[X])[:, 1])\n",
    "\n",
    "data_ps[[\"T_binary\", \"res\", \"propensity_score\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_t = 1/data_ps.query(\"T_binary==1\")[\"propensity_score\"]\n",
    "weight_nt = 1/(1-data_ps.query(\"T_binary==0\")[\"propensity_score\"])\n",
    "print(\"Original Sample Size\", df.shape[0])\n",
    "print(\"Treated Population Sample Size\", sum(weight_t))\n",
    "print(\"Untreated Population Sample Size\", sum(weight_nt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.distplot(data_ps.query(\"T_binary==0\")[\"propensity_score\"], kde=False, label=\"Non Treated\")\n",
    "sns.distplot(data_ps.query(\"T_binary==1\")[\"propensity_score\"], kde=False, label=\"Treated\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove observations with propensity score = 1\n",
    "data_ps = data_ps[data_ps[\"propensity_score\"] < 1]\n",
    "\n",
    "\n",
    "treated_data = data_ps.query(\"T_binary == 1\")\n",
    "control_data = data_ps.query(\"T_binary == 0\")\n",
    "\n",
    "y1 = sum(treated_data[\"T_binary\"] * weight_t) / len(treated_data)\n",
    "y0 = sum(control_data[\"T_binary\"] * weight_nt) / len(control_data)\n",
    "\n",
    "ate = np.mean(weight_t * treated_data[\"T_binary\"]) - np.mean(weight_nt * control_data[\"T_binary\"])\n",
    "\n",
    "print(ate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate weights for treated and control groups\n",
    "treated_data = data_ps.query(\"res == 1\")\n",
    "control_data = data_ps.query(\"res == 0\")\n",
    "\n",
    "if not treated_data.empty:\n",
    "    weight_t = 1 / treated_data[\"propensity_score\"]\n",
    "    print(\"Weight_t:\", weight_t)\n",
    "\n",
    "if not control_data.empty:\n",
    "    weight_nt = 1 / (1 - control_data[\"propensity_score\"])\n",
    "    print(\"Weight_nt:\", weight_nt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_ps[\"propensity_score\"].min(), data_ps[\"propensity_score\"].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ps(df_final, X, T, y):\n",
    "    ps = LogisticRegression(C=1e6, max_iter=2000, solver='liblinear').fit(df_final[X], df_final[T]).predict_proba(df_final[X])[:, 1]\n",
    "    weight = (df_final[T]-ps) / (ps*(1-ps)) \n",
    "    return np.mean(weight * df_final[y]) \n",
    "\n",
    "sample_df = df_final.sample(frac=1, replace=True)\n",
    "ate = run_ps(sample_df, X, T, Y)\n",
    "print(ate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = df_final.sample(frac=0.1, replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ps(sample_df, X, T, y):\n",
    "    ps = LogisticRegression(C=1e6, max_iter=2000, solver='liblinear').fit(sample_df[X], sample_df[T]).predict_proba(sample_df[X])[:, 1]\n",
    "    weight = (sample_df[T]-ps) / (ps*(1-ps)) \n",
    "    return np.mean(weight * sample_df[y])\n",
    "\n",
    "np.random.seed(88)\n",
    "df_sampled = df_final.sample(frac=0.1, replace=True) \n",
    "bootstrap_sample = 100\n",
    "ates = Parallel(n_jobs=4)(delayed(run_ps)(df_sampled.sample(frac=1, replace=True), X, T, Y)\n",
    "                          for _ in range(bootstrap_sample))\n",
    "ates = np.array(ates)\n",
    "ates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(ates, kde=False)\n",
    "plt.vlines(np.percentile(ates, 2.5), 0, 30, linestyles=\"dotted\")\n",
    "plt.vlines(np.percentile(ates, 97.5), 0, 30, linestyles=\"dotted\", label=\"95% CI\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learners "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code in this section based on the notebook: CIBT-21 Meta-Learners\n",
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df_final, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "s_learner = LGBMRegressor(max_depth=3, min_child_samples=30)\n",
    "s_learner.fit(df_train[X+[T]], df_train[Y]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_learner_cate_train = (s_learner.predict(df_train[X].assign(**{T: 1})) -\n",
    "                        s_learner.predict(df_train[X].assign(**{T: 0})))\n",
    "\n",
    "s_learner_cate_test = df_test.assign(\n",
    "    cate=(s_learner.predict(df_test[X].assign(**{T: 1})) - \n",
    "          s_learner.predict(df_test[X].assign(**{T: 0}))) \n",
    ")\n",
    "ATE_test = s_learner_cate_test['cate'].mean()\n",
    "print(ATE_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(df_test['chair'], s_learner_cate_test['cate'], alpha=0.5, color='lightblue')\n",
    "plt.xlabel('Judge Identification')\n",
    "plt.ylabel('CATE')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elast(data, y, t):\n",
    "    return (np.sum((data[t] - data[t].mean())*(data[y] - data[y].mean())) /\n",
    "            np.sum((data[t] - data[t].mean())**2))\n",
    "\n",
    "def cumulative_gain(dataset, prediction, y, t, min_periods=30, steps=100):\n",
    "    size = dataset.shape[0]\n",
    "    ordered_df = dataset.sort_values(prediction, ascending=False).reset_index(drop=True)\n",
    "    n_rows = list(range(min_periods, size, size // steps)) + [size]\n",
    "    return np.array([elast(ordered_df.head(rows), y, t) * (rows/size) for rows in n_rows])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gain_curve_test = cumulative_gain(s_learner_cate_test, \"cate\", y=\"res\", t=\"T_binary\")\n",
    "gain_curve_train = cumulative_gain(df_train.assign(cate=s_learner_cate_train), \"cate\",y=\"res\", t=\"T_binary\")\n",
    "plt.plot(gain_curve_test, color=\"C0\", label=\"Test\")\n",
    "plt.plot(gain_curve_train, color=\"C1\", label=\"Train\")\n",
    "plt.plot([0, 100], [0, elast(df_test, y=\"res\", t=\"T_binary\")], linestyle=\"--\", color=\"black\", label=\"Baseline\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ate_s_learner(df, X, T, Y):\n",
    "    s_learner = LGBMRegressor(max_depth=3, min_child_samples=30)\n",
    "    s_learner.fit(df[X + [T]], df[Y])\n",
    "    cate = (\n",
    "        s_learner.predict(df[X].assign(**{T: 1})) -\n",
    "        s_learner.predict(df[X].assign(**{T: 0}))\n",
    "    )\n",
    "    return cate.mean()\n",
    "\n",
    "bootstrap_samples = 1000\n",
    "\n",
    "df_sampled = df_test.sample(frac=0.1, replace=True)\n",
    "\n",
    "np.random.seed(88)  \n",
    "ates_s_learner = Parallel(n_jobs=4)(delayed(compute_ate_s_learner)(\n",
    "    df_sampled.sample(frac=1, replace=True), X, T, Y\n",
    ") for _ in range(bootstrap_samples))\n",
    "\n",
    "ates_s_learner = np.array(ates_s_learner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(ates_s_learner, kde=False)\n",
    "plt.vlines(np.percentile(ates_s_learner, 2.5), 0, 20, linestyles=\"dotted\")\n",
    "plt.vlines(np.percentile(ates_s_learner, 97.5), 0, 20, linestyles=\"dotted\", label=\"95% CI\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T-Learner "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "m0 = LGBMRegressor(max_depth=2, min_child_samples=60)\n",
    "m1 = LGBMRegressor(max_depth=2, min_child_samples=60)\n",
    "\n",
    "m0.fit(df_train.query(f\"{T}==0\")[X], df_train.query(f\"{T}==0\")[Y])\n",
    "m1.fit(df_train.query(f\"{T}==1\")[X], df_train.query(f\"{T}==1\")[Y])\n",
    "\n",
    "t_learner_cate_train = m1.predict(df_train[X]) - m0.predict(df_train[X])\n",
    "t_learner_cate_test = df_test.assign(cate=m1.predict(df_test[X]) - m0.predict(df_test[X]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ATE_test_t = t_learner_cate_test['cate'].mean()\n",
    "print(ATE_test_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(df_test['chair'], t_learner_cate_test['cate'], alpha=0.5, color='lightblue')\n",
    "plt.xlabel('Judge Identification')\n",
    "plt.ylabel('CATE')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gain_curve_test = cumulative_gain(t_learner_cate_test, \"cate\", y=\"res\", t=\"T_binary\")\n",
    "gain_curve_train = cumulative_gain(df_train.assign(cate=t_learner_cate_train), \"cate\", y=\"res\", t=\"T_binary\")\n",
    "plt.plot(gain_curve_test, color=\"C0\", label=\"Test\")\n",
    "plt.plot(gain_curve_train, color=\"C1\", label=\"Train\")\n",
    "plt.plot([0, 100], [0, elast(df_test, \"res\", \"T_binary\")], linestyle=\"--\", color=\"black\", label=\"Baseline\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ate_bootstrap(df, X, T, Y, m0, m1):\n",
    "    m0.fit(df.query(f\"{T}==0\")[X], df.query(f\"{T}==0\")[Y])\n",
    "    m1.fit(df.query(f\"{T}==1\")[X], df.query(f\"{T}==1\")[Y])\n",
    "    cate = m1.predict(df[X]) - m0.predict(df[X])\n",
    "    return cate.mean()\n",
    "\n",
    "bootstrap_samples = 1000\n",
    "\n",
    "df_sampled = df_test.sample(frac=0.1, replace=True)\n",
    "\n",
    "np.random.seed(88)  #\n",
    "ates_tlearner = Parallel(n_jobs=4)(delayed(compute_ate_bootstrap)(\n",
    "df_sampled.sample(frac=1, replace=True), X, T, Y, m0, m1\n",
    ") for _ in range(bootstrap_samples))\n",
    "\n",
    "ates_tlearner = np.array(ates_tlearner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(ates_tlearner, kde=False)\n",
    "plt.vlines(np.percentile(ates_tlearner, 2.5), 0, 20, linestyles=\"dotted\")\n",
    "plt.vlines(np.percentile(ates_tlearner, 97.5), 0, 20, linestyles=\"dotted\", label=\"95% CI\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X-Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "m0 = LGBMRegressor(max_depth=2, min_child_samples=30)\n",
    "m1 = LGBMRegressor(max_depth=2, min_child_samples=30)\n",
    "\n",
    "g = LogisticRegression(solver=\"lbfgs\", penalty='none') \n",
    "\n",
    "m0.fit(df_train.query(f\"{T}==0\")[X], df_train.query(f\"{T}==0\")[Y])\n",
    "m1.fit(df_train.query(f\"{T}==1\")[X], df_train.query(f\"{T}==1\")[Y])\n",
    "                       \n",
    "g.fit(df_train[X], df_train[T]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train = np.where(df_train[T]==0,\n",
    "                   m1.predict(df_train[X]) - df_train[Y],\n",
    "                   df_train[Y] - m0.predict(df_train[X]))\n",
    "\n",
    "mx0 = LGBMRegressor(max_depth=2, min_child_samples=30)\n",
    "mx1 = LGBMRegressor(max_depth=2, min_child_samples=30)\n",
    "\n",
    "mx0.fit(df_train.query(f\"{T}==0\")[X], d_train[df_train[T]==0])\n",
    "mx1.fit(df_train.query(f\"{T}==1\")[X], d_train[df_train[T]==1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ps_predict(df, t): \n",
    "    return g.predict_proba(df[X])[:, t]\n",
    "    \n",
    "    \n",
    "x_cate_train = (ps_predict(df_train,1)*mx0.predict(df_train[X]) +\n",
    "                ps_predict(df_train,0)*mx1.predict(df_train[X]))\n",
    "\n",
    "x_cate_test = df_test.assign(cate=(ps_predict(df_test,1)*mx0.predict(df_test[X]) +\n",
    "                                ps_predict(df_test,0)*mx1.predict(df_test[X])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ATE_test_x = x_cate_test['cate'].mean()\n",
    "print(ATE_test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(df_test['chair'], x_cate_test['cate'], alpha=0.5, color='lightblue')\n",
    "plt.xlabel('Judge Identification')\n",
    "plt.ylabel('CATE')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ate_x_learner(df, X, T, Y):\n",
    "    m0 = LGBMRegressor(max_depth=2, min_child_samples=30)\n",
    "    m1 = LGBMRegressor(max_depth=2, min_child_samples=30)\n",
    "    m0.fit(df.query(f\"{T}==0\")[X], df.query(f\"{T}==0\")[Y])\n",
    "    m1.fit(df.query(f\"{T}==1\")[X], df.query(f\"{T}==1\")[Y])\n",
    "    \n",
    "    d_train = np.where(\n",
    "        df[T] == 0,\n",
    "        m1.predict(df[X]) - df[Y],\n",
    "        df[Y] - m0.predict(df[X])\n",
    "    )\n",
    "    \n",
    "    g = LogisticRegression(solver=\"lbfgs\", penalty='none')\n",
    "    g.fit(df[X], df[T])\n",
    "    \n",
    "    def ps_predict(data, t):\n",
    "        return g.predict_proba(data[X])[:, t]\n",
    "    \n",
    "    mx0 = LGBMRegressor(max_depth=2, min_child_samples=30)\n",
    "    mx1 = LGBMRegressor(max_depth=2, min_child_samples=30)\n",
    "    \n",
    "    mx0.fit(df.query(f\"{T}==0\")[X], d_train[df[T] == 0])\n",
    "    mx1.fit(df.query(f\"{T}==1\")[X], d_train[df[T] == 1])\n",
    "    \n",
    "    cate = (ps_predict(df, 1) * mx0.predict(df[X]) +\n",
    "            ps_predict(df, 0) * mx1.predict(df[X]))\n",
    "    \n",
    "    return cate.mean()\n",
    "\n",
    "bootstrap_samples = 100\n",
    "\n",
    "df_sampled = df_test.sample(frac=0.1, replace=True)\n",
    "\n",
    "np.random.seed(123)  \n",
    "x_ates_x = Parallel(n_jobs=4)(delayed(compute_ate_x_learner)(\n",
    "    df_sampled.sample(frac=1, replace=True), X, T, Y\n",
    ") for _ in range(bootstrap_samples))\n",
    "\n",
    "x_ates_x_learner = np.array(x_ates_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(x_ates_x_learner, kde=False)\n",
    "plt.vlines(np.percentile(x_ates_x_learner, 2.5), 0, 20, linestyles=\"dotted\")\n",
    "plt.vlines(np.percentile(x_ates_x_learner, 97.5), 0, 20, linestyles=\"dotted\", label=\"95% CI\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gain_curve_test = cumulative_gain(x_cate_test, \"cate\", y=\"res\", t=\"T_binary\")\n",
    "gain_curve_train = cumulative_gain(df_train.assign(cate=x_cate_train), \"cate\", y=\"res\", t=\"T_binary\")\n",
    "plt.plot(gain_curve_test, color=\"C0\", label=\"Test\")\n",
    "plt.plot(gain_curve_train, color=\"C1\", label=\"Train\")\n",
    "plt.plot([0, 100], [0, elast(df_test, \"res\", \"T_binary\")], linestyle=\"--\", color=\"black\", label=\"Baseline\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doubly Robust "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first half of the code based on: CIBT-12-Doubly-Robust-Estimation\n",
    "def doubly_robust(df, X, T, Y):\n",
    "    ps = LogisticRegression(C=1e6, max_iter=1000).fit(df[X], df[T]).predict_proba(df[X])[:, 1]\n",
    "    mu0 = LinearRegression().fit(df.query(f\"{T}==0\")[X], df.query(f\"{T}==0\")[Y]).predict(df[X])\n",
    "    mu1 = LinearRegression().fit(df.query(f\"{T}==1\")[X], df.query(f\"{T}==1\")[Y]).predict(df[X])\n",
    "    return (\n",
    "        np.mean(df[T]*(df[Y] - mu1)/ps + mu1) -\n",
    "        np.mean((1-df[T])*(df[Y] - mu0)/(1-ps) + mu0)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doubly_robust(df_final, X, T, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(88)\n",
    "bootstrap_sample = 100\n",
    "df_sampled = df_final.sample(frac=0.1, replace=True)\n",
    "ates_double = Parallel(n_jobs=4)(delayed(doubly_robust)(df_sampled.sample(frac=1, replace=True), X, T, Y)\n",
    "                          for _ in range(bootstrap_sample))\n",
    "ates_double = np.array(ates_double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(ates_double, kde=False)\n",
    "plt.vlines(np.percentile(ates_double, 2.5), 0, 20, linestyles=\"dotted\")\n",
    "plt.vlines(np.percentile(ates_double, 97.5), 0, 20, linestyles=\"dotted\", label=\"95% CI\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code based on Doubly Robust Learner and Interpretability-econml notebook\n",
    "\n",
    "est = LinearDRLearner(model_regression=LassoCV(cv=3),\n",
    "                      model_propensity= LogisticRegression(C=1e6, max_iter=1000, solver='liblinear'))\n",
    "\n",
    "Y_value = df_final[Y]\n",
    "T_value = df_final[T]\n",
    "X_value = df_final[X]\n",
    "\n",
    "\n",
    "est.fit(Y_value, T_value, X=X_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est.fit(Y_value, T_value, X=X_value)\n",
    "\n",
    "treatment_effects = est.effect(X_value)  \n",
    "\n",
    "average_treatment_effect = np.mean(treatment_effects)\n",
    "\n",
    "print(average_treatment_effect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = est.shap_values(X_value)\n",
    "shap_values_for_target = shap_values['res']['T_binary_1'].values  \n",
    "\n",
    "shap_values_for_target = shap_values_for_target.astype(float)\n",
    "\n",
    "if np.any(np.isnan(shap_values_for_target)) or np.any(np.isinf(shap_values_for_target)):\n",
    "    print(\"NaN or Inf values found in SHAP values, replacing them with zeros.\")\n",
    "    shap_values_for_target = np.nan_to_num(shap_values_for_target)  \n",
    "\n",
    "print(f\"Shape of shap_values_for_target after adjustments: {shap_values_for_target.shape}\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "shap_values_scaled = scaler.fit_transform(shap_values_for_target)\n",
    "\n",
    "shap.summary_plot(shap_values_scaled, feature_names=[\n",
    "    'chair', 'dummy_asylum', 'dummy_gender', 'middleast', 'america', 'africa', 'europe', \n",
    "    'northeast', 'midwest', 'south', 'year2000', 'year2001', 'year2002', 'year2003', \n",
    "    'month_1', 'month_2', 'month_3', 'month_4', 'month_5', 'month_6', 'month_7', \n",
    "    'month_8', 'month_9', 'month_10', 'month_11'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = CausalForestDML(model_y=RandomForestRegressor(n_estimators=50),\n",
    "                      model_t=RandomForestClassifier(n_estimators=50),  \n",
    "                      discrete_treatment=True)  \n",
    "\n",
    "est.fit(Y_value, T_value, X=X_value)\n",
    "\n",
    "treatment_effects = est.effect(X_value)\n",
    "\n",
    "print(treatment_effects.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(88)\n",
    "bootstrap_sample = 10\n",
    "ates_double_causal = Parallel(n_jobs=4)(delayed(est)(sample_df.sample(frac=1, replace=True), X, T, Y)\n",
    "                          for _ in range(bootstrap_sample))\n",
    "ates_double_causal = np.array(ates_double_causal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(ates_double, kde=False)\n",
    "plt.vlines(np.percentile(ates_double_causal, 2.5), 0, 20, linestyles=\"dotted\")\n",
    "plt.vlines(np.percentile(ates_double_causal, 97.5), 0, 20, linestyles=\"dotted\", label=\"95% CI\")\n",
    "plt.legend();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
