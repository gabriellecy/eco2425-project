{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2 - File for Project 1 Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data from: Heyes, Anthony, and Soodeh Saberian. 2019. \"Temperature and Decisions: Evidence from 207,000 Court Cases.\" American Economic Journal: Applied Economics, 11 (2): 238–65.\n",
    "\n",
    "Notebooks used troughout the code: \n",
    "- ISLP-Ch06_varselect_lab.ipynb\n",
    "- ISLP-TreeModels.ipynb\n",
    "- CIDP-Chapter_04\n",
    "- CIDP-Chapter_05\n",
    "- CIDP-Chapter_07\n",
    "- CIBT-11-Propensity-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import sklearn.linear_model as skl\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sklearn.model_selection as skm\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from matplotlib.pyplot import subplots\n",
    "from statsmodels.discrete.discrete_model import Probit\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "!pip install stargazer\n",
    "from stargazer.stargazer import Stargazer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import (DecisionTreeClassifier as DTC,\n",
    "                          DecisionTreeRegressor as DTR,\n",
    "                          plot_tree,\n",
    "                          export_text)\n",
    "from sklearn.metrics import (accuracy_score,\n",
    "                             log_loss)\n",
    "from sklearn.ensemble import \\\n",
    "     (RandomForestClassifier as RFC,\n",
    "      GradientBoostingClassifier as GBC)\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "import graphviz\n",
    "import networkx as nx\n",
    "COLORS = [\n",
    "    '#00B0F0',\n",
    "    '#FF0000'\n",
    "]\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.ticker import FuncFormatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install dowhy\n",
    "import dowhy\n",
    "from dowhy import CausalModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LassoCV\n",
    "!pip install econml\n",
    "from econml.dr import LinearDRLearner\n",
    "from joblib import Parallel, delayed \n",
    "import shap\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from econml.dml import CausalForestDML\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Description "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_stata('matched_corrected.dta')\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dummy for asylum\n",
    "df['dummy_asylum'] = df['c_asy_type'].apply(lambda x: 1 if x == 'E' else 0)\n",
    "#Create a dummy for gender\n",
    "df['dummy_gender'] = df['gender'].apply(lambda x: 1 if x == 'female' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As outlined in the correction article drop the observation for China\n",
    "df = df[df['nat_name'] != 'CHINA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique values to identify variables for the dummy variables\n",
    "unique__names = df['nat_name'].unique()\n",
    "locations = df['location'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of regions\n",
    "middle_eastern_countries = [\"BAHRAIN\", \"CYPRUS\", \"EGYPT\", \"IRAN\", \"IRAQ\", \"ISRAEL\", \"JORDAN\", \n",
    "    \"KUWAIT\", \"LEBANON\", \"OMAN\", \"PALESTINE\", \"QATAR\", \"SAUDI ARABIA\", \n",
    "    \"SYRIA\", \"TURKEY\", \"UNITED ARAB EMIRATES\", \"YEMEN\"]\n",
    "\n",
    "africa = [\"ERITREA\", \"RWANDA\", \"SOMALIA\", \"SUDAN\", \"CONGO\", \"ETHIOPIA\", \"LIBYA\", \n",
    "    \"MALI\", \"ANGOLA\", \"BURUNDI\", \"TANZANIA\", \"NIGERIA\", \"GABON\", \"GHANA\", \n",
    "    \"SENEGAL\", \"CHAD\", \"DJIBOUTI\", \"CAMEROON\", \"UGANDA\", \"KENYA\", \n",
    "    \"ZAMBIA\", \"MAURITANIA\", \"SOUTH AFRICA\", \"GUINEA\", \"BURKINA FASO\", \n",
    "    \"MOROCCO\", \"ALGERIA\", \"COMORO ISLANDS\", \"EQUATORIAL GUINEA\", \n",
    "    \"CENTRAL AFRICAN REPUBLIC\", \"CAPE VERDE\", \"LESOTHO\", \"SWAZILAND\", \n",
    "    \"GAMBIA\", \"SIERRA LEONE\", \"GUINEA BISSAU\"]\n",
    "\n",
    "america = [\"GUATEMALA\", \"EL SALVADOR\", \"PANAMA\", \"COLOMBIA\", \n",
    "    \"ARGENTINA\", \"HAITI\", \"VENEZUELA\", \"MEXICO\", \"CUBA\", \"DOMINICAN REPUBLIC\", \n",
    "    \"BRAZIL\", \"CHILE\", \"SURINAME\", \"TRINIDAD AND TOBAGO\", \"JAMAICA\", \n",
    "    \"CANADA\", \"USA\", \"ST. KITTS, WEST INDIES\", \"ANTIGUA AND BARBUDA\", \n",
    "    \"BARBADOS\", \"BAHAMAS\", \"BELIZE\", \"DOMINICA\", \"GRENADA\", \n",
    "    \"NICARAGUA\", \"URUGUAY\", \"PARAGUAY\", \"ST. LUCIA\", \"ST. VINCENT AND THE GRENADINES\"]\n",
    "\n",
    "asia = [\"PAKISTAN\", \"VIETNAM\", \"INDONESIA\", \"AFGHANISTAN\", \n",
    "    \"IRAN\", \"BANGLADESH\", \"PHILIPPINES\", \"TAIWAN\", \"MALAYSIA\", \n",
    "    \"KAZAKHSTAN\", \"KYRGYZSTAN\", \"THAILAND\", \"TURKMENISTAN\", \"UZBEKISTAN\", \n",
    "    \"MONGOLIA\", \"SRI LANKA\", \"BHUTAN\", \"LAOS\", \"NEPAL\", \n",
    "    \"MYANMAR\", \"KAMPUCHEA\", \"BRUNEI\", \"BURMA\", \"KOREA\", \"NORTH KOREA\"]\n",
    "\n",
    "europe = [\"RUSSIA\", \"ARMENIA\", \"ALBANIA\", \"YUGOSLAVIA\", \"UNITED KINGDOM\", \n",
    "    \"BULGARIA\", \"ROMANIA\", \"HUNGARY\", \"POLAND\", \"CZECH REPUBLIC\", \n",
    "    \"SLOVAK REPUBLIC\", \"GERMANY\", \"FRANCE\", \"ITALY\", \"SPAIN\", \n",
    "    \"SWEDEN\", \"DENMARK\", \"FINLAND\", \"AUSTRIA\", \"SWITZERLAND\", \n",
    "    \"BELGIUM\", \"GREECE\", \"NETHERLANDS\", \"CROATIA\", \"SLOVENIA\", \n",
    "    \"MONACO\", \"LITHUANIA\", \"LATVIA\", \"ESTONIA\", \"ICELAND\"]\n",
    "\n",
    "df['middleast'] = 0\n",
    "df['america'] = 0\n",
    "df['africa'] = 0\n",
    "df['asia'] = 0\n",
    "df['europe'] = 0\n",
    "\n",
    "df.loc[df['nat_name'].isin(middle_eastern_countries), 'middleast'] = 1\n",
    "df.loc[df['nat_name'].isin(america), 'america'] = 1\n",
    "df.loc[df['nat_name'].isin(africa), 'africa'] = 1\n",
    "df.loc[df['nat_name'].isin(asia), 'asia'] = 1\n",
    "df.loc[df['nat_name'].isin(europe), 'europe'] = 1\n",
    "\n",
    "#Create interaction terms\n",
    "df['middleast_dev'] = df['middleast']*df['temp6t4']\n",
    "df['america_dev'] = df['america']*df['temp6t4']\n",
    "df['africa_dev'] = df['africa']*df['temp6t4']\n",
    "df['asia_dev'] = df['asia']*df['temp6t4']\n",
    "df['europe_dev'] = df['europe']*df['temp6t4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define list of locations\n",
    "northeast = ['NEWARK', 'BOSTON', 'NEW YORK CITY', 'BUFFALO', 'PHILADELPHIA', \n",
    "    'NEW YORK ANNEX', 'NY DET (VARICK ST.)', 'HARTFORD', \n",
    "    '*PA DOC.', 'CLEVELAND', '*BOP  DANBURY', '*RI  DOC',\n",
    "    '*WISCONSIN DOC', '*NH  DOC', '*SUFFOLK COUNTY','*NEWARK VIDEO HEARINGS','*JESSUP'\n",
    "    '*BOP ALLENWOOD', '*NORTHERN STATE NJ DOC','YORK COUNTY DET','YORK COUNTY DET']\n",
    "\n",
    "midwest = ['CHICAGO', 'DETROIT', 'CINCINNATI', 'CLEVELAND', 'ST. LOUIS', \n",
    "    'MEMPHIS', 'KANSAS CITY', 'OMAHA', '*MI  DOC', \n",
    "    '*IL DOC - STATESVILLE', '*MO DOC', '*OHIO DOC', \n",
    "    '*INDIANA YOUTH CENTER']\n",
    "\n",
    "south = ['ARLINGTON', 'DALLAS', 'HOUSTON', 'MIAMI', 'ATLANTA', \n",
    "    'NEW ORLEANS', 'SAN ANTONIO', 'DALLAS DET', 'SAN ANTONIO DET', \n",
    "    'HOUSTON DET', 'ATLANTA DET', '*GEORGIA DOC', '*VA DOC', \n",
    "    '*DADE COUNTY FL DOC', '*BROWARD  FL DOC', 'ORLANDO', 'KROME DET',\n",
    "    'PORT ISABEL DET', 'EL PASO', 'EL PASO DET', '*TX DOC', \n",
    "    'LOUISVILLE', 'OKLAHOMA CITY', 'OKLAHOMA CITY DET', \n",
    "    'BATAVIA SPC', 'BROWARD TRANS CTR','ST. THOMAS', 'ST. CROIX', 'ROLLING PLAINS DETENTION CENTER',\n",
    "    '*BOP BIG SPRING AIRPARK','BRADENTON DET','SAN ANTONIO DET']\n",
    "\n",
    "west = ['DENVER', 'SAN DIEGO', 'LOS ANGELES', 'SAN FRANCISCO', \n",
    "    'PHOENIX', 'LAS VEGAS', 'RENO', 'SALT LAKE CITY', 'OTAY MESA', \n",
    "    'TUCSON', 'HONOLULU', 'SAN JUAN', 'SEATTLE', 'PORTLAND',\n",
    "    'SAN FRANCISCO DET', 'DENVER DET', 'SAN DIEGO DETAINED', \n",
    "    'MIRA LOMA DET', 'HONOLULU DET', '*CO DOC', '*AZ DOC',\n",
    "    '*WA DOC', '*AK DOC', 'ANCHORAGE', 'SAN PEDRO', \n",
    "    'IMPERIAL', '*NM DOC','PORTLAND DET','*MONROE WA DOC','SAN FRANCISCO ANNEX']\n",
    "\n",
    "\n",
    "df['northeast'] = 0\n",
    "df['midwest'] = 0\n",
    "df['south'] = 0\n",
    "df['west'] = 0\n",
    "\n",
    "df.loc[df['location'].isin(northeast), 'northeast'] = 1\n",
    "df.loc[df['location'].isin(midwest), 'midwest'] = 1\n",
    "df.loc[df['location'].isin(south), 'south'] = 1\n",
    "df.loc[df['location'].isin(west), 'west'] = 1\n",
    "\n",
    "#Create interaction terms\n",
    "df['middleast_dev'] = df['middleast']*df['temp6t4']\n",
    "df['america_dev'] = df['america']*df['temp6t4']\n",
    "df['africa_dev'] = df['africa']*df['temp6t4']\n",
    "df['asia_dev'] = df['asia']*df['temp6t4']\n",
    "df['europe_dev'] = df['europe']*df['temp6t4']\n",
    "\n",
    "locations = ['northeast', 'midwest', 'south', 'west']\n",
    "nationalities = ['middleast', 'america', 'africa', 'asia', 'europe']\n",
    "\n",
    "for nationality in nationalities:\n",
    "    for location in locations:\n",
    "        interaction_column_name = f'{location}_{nationality}_dev'\n",
    "        df[interaction_column_name] = df[location] * df[nationality] * df['temp6t4']\n",
    "        \n",
    "for location in locations:\n",
    "    interaction_column_name = f'{location}_dev'\n",
    "    df[interaction_column_name] = df[location] * df['temp6t4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy variables for the months \n",
    "df['month'] = df['date'].dt.month\n",
    "df = pd.get_dummies(df, columns=['month'], prefix='month', drop_first=False)\n",
    "\n",
    "#Dummy for winter and summer\n",
    "df['month'] = df['date'].dt.month\n",
    "\n",
    "df['summer_spring'] = df['month'].isin([3, 4, 5, 6, 7, 8]).astype(int)\n",
    "df['winter_fall'] = df['month'].isin([1, 2, 9, 10, 11, 12]).astype(int)\n",
    "\n",
    "seasons = ['summer_spring', 'winter_fall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean the dataset\n",
    "df = df[df['c_asy_type'].isin(['E', 'I'])]\n",
    "df_final = df.dropna(axis=0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_stats = df_final[['temp6t4','heat','wind6t4','skycover','ozone','pm']].describe() \n",
    "print(summary_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_temp = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]  \n",
    "labels = [f\"{range_temp[i]}-{range_temp[i+1]}\" for i in range(len(range_temp) - 1)]  \n",
    "\n",
    "df_final['temp_bins'] = pd.cut(df_final['temp6t4'], bins=range_temp, labels=labels, right=False)\n",
    "\n",
    "counts = df_final['temp_bins'].value_counts().sort_index()\n",
    "percentages = counts / counts.sum() * 100\n",
    "\n",
    "plt.figure(figsize=(7, 6), facecolor='white')  \n",
    "ax = plt.subplot(111, facecolor='white')  \n",
    "percentages.plot(kind='bar', color='skyblue')\n",
    "plt.xlabel('Temperature (°F)')\n",
    "plt.ylabel('Percentage of Cases')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y')\n",
    "\n",
    "ax.yaxis.set_major_formatter(FuncFormatter(lambda y, _: f'{y:.0f}%'))\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_visible(False)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.array(df_final['res'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create X variables for different specifications\n",
    "#Note: drop one category for each dummy\n",
    "\n",
    "#Specification 1\n",
    "selectedvariables = ['temp6t4','heat','skycover', 'co', 'co_distance', 'ozone', 'ozone_distance', 'pm', \n",
    "                     'pm_distance', 'press6t4', 'dew6t4', 'prcp6t4', 'wind6t4', \n",
    "                     'rh6t4', 'chair', 'dummy_asylum', 'dummy_gender', \n",
    "                     'middleast', 'america', 'africa', 'europe', 'northeast', 'midwest', \n",
    "                     'south', 'year2000', 'year2001', 'year2002', \n",
    "                     'year2003','middleast_dev','america_dev','africa_dev','europe_dev','month_1',\n",
    "                     'month_2','month_3','month_4','month_5','month_6','month_7','month_8',\n",
    "                     'month_9','month_10','month_11']\n",
    "\n",
    "#Specification 2\n",
    "selectedvariables_noweather = ['temp6t4','chair', 'dummy_asylum', 'dummy_gender', \n",
    "                     'middleast', 'america', 'africa', 'europe', 'northeast', 'midwest', \n",
    "                     'south', 'year2000', 'year2001', 'year2002', \n",
    "                     'year2003','middleast_dev','america_dev','africa_dev','europe_dev','month_1','month_2',\n",
    "                      'month_3','month_4','month_5','month_6','month_7','month_8',\n",
    "                    'month_9','month_10','month_11']\n",
    "\n",
    "# Deviation Specification \n",
    "selectedvariables_deviation = ['deviation','heat','skycover', 'co', 'co_distance', 'ozone', 'ozone_distance', 'pm', \n",
    "                     'pm_distance', 'press6t4', 'dew6t4', 'prcp6t4', 'wind6t4', \n",
    "                     'rh6t4', 'chair', 'dummy_asylum', 'dummy_gender', \n",
    "                     'middleast', 'america', 'africa', 'europe', 'northeast', 'midwest', \n",
    "                     'south', 'year2000', 'year2001', 'year2002', \n",
    "                     'year2003','middleast_dev','america_dev','africa_dev','europe_dev','month_1',\n",
    "                     'month_2','month_3','month_4','month_5','month_6','month_7','month_8',\n",
    "                     'month_9','month_10','month_11']\n",
    "                        \n",
    "#Create X variables with different specification\n",
    "X = df_final[selectedvariables]\n",
    "\n",
    "X_no_control = df_final[selectedvariables_noweather]\n",
    "\n",
    "X_deviation = df_final[selectedvariables_deviation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specification 1\n",
    "model_1 = Probit(Y, X.astype(float))\n",
    "probit_model1 = model_1.fit()\n",
    "print(probit_model1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate marginal effect for the variable of interest\n",
    "predicted_probs = probit_model1.predict(X.astype(float))\n",
    "\n",
    "x_temp6t4 = X['temp6t4']  \n",
    "marginal_effect_temp = probit_model1.params['temp6t4'] * predicted_probs * (1 - predicted_probs)\n",
    "average_marginal_effect_temp = np.mean(marginal_effect_temp)\n",
    "\n",
    "print(average_marginal_effect_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stargazer = Stargazer([probit_model1])\n",
    "print(stargazer.render_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specification 1 - Deviation \n",
    "\n",
    "model_1_deviation = Probit(Y, X_deviation.astype(float))\n",
    "probit_model1_deviation = model_1_deviation.fit()\n",
    "print(probit_model1_deviation.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stargazer_dev = Stargazer([probit_model1_deviation])\n",
    "print(stargazer_dev.render_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specification 2\n",
    "model_2 = Probit(Y, X_no_control.astype(float))\n",
    "probit_model2 = model_2.fit()\n",
    "print(probit_model2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stargazer_2 = Stargazer([probit_model2])\n",
    "print(stargazer_2.render_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate marginal effect for the variable of interest \n",
    "predicted_probs = probit_model2.predict(X_no_control.astype(float))\n",
    "x_temp6t4 = X['temp6t4']  \n",
    "marginal_effect_temp = probit_model2.params['temp6t4'] * predicted_probs * (1 - predicted_probs)\n",
    "average_marginal_effect_temp = np.mean(marginal_effect_temp)\n",
    "\n",
    "print(average_marginal_effect_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter per month \n",
    "df_final['date'] = pd.to_datetime(df_final['date'])\n",
    "\n",
    "#Filter the dataset as winter/fall and summer/spring\n",
    "df_filter_nowinter = df_final[df_final['date'].dt.month.isin([3, 4, 5, 6, 7, 8])]\n",
    "df_filter_winter = df_final[df_final['date'].dt.month.isin([1,2,9,10,11,12])] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average and standard deviation of resolution winter and non-winter months\n",
    "winter_mean = df_filter_winter['res'].mean()\n",
    "winter_std = df_filter_winter['res'].std()\n",
    "nowinter_mean = df_filter_nowinter['res'].mean()\n",
    "nowinter_std = df_filter_nowinter['res'].std()\n",
    "\n",
    "summary = pd.DataFrame({\n",
    "    'season': ['Winter and Fall', 'Summer and Spring'],\n",
    "    'mean': [winter_mean, nowinter_mean],\n",
    "    'std_dev': [winter_std, nowinter_std]\n",
    "})\n",
    "\n",
    "plt.figure(figsize=(7, 7), facecolor='white')\n",
    "ax = plt.subplot(111, facecolor='white')\n",
    "ax.grid(False) \n",
    "\n",
    "bars = plt.bar(summary['season'], summary['mean'], yerr=summary['std_dev'], capsize=5, color=['skyblue', 'lightcoral'])\n",
    "plt.ylabel('Average Resolution')\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_visible(False)\n",
    "\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average and standard deviation of deviation of average temp for winter and non-winter months\n",
    "winter_mean_temp = df_filter_winter['deviation'].mean()\n",
    "winter_std_temp = df_filter_winter['deviation'].std()\n",
    "nowinter_mean_temp = df_filter_nowinter['deviation'].mean()\n",
    "nowinter_std_temp = df_filter_nowinter['deviation'].std()\n",
    "\n",
    "summary_temp = pd.DataFrame({\n",
    "    'season': ['Winter and Fall', 'Summer and Spring'],\n",
    "    'mean': [winter_mean_temp, nowinter_mean_temp],\n",
    "    'std_dev': [winter_std_temp, nowinter_std_temp]\n",
    "})\n",
    "\n",
    "plt.figure(figsize=(7, 7), facecolor='white')\n",
    "ax = plt.subplot(111, facecolor='white')\n",
    "ax.grid(False) \n",
    "\n",
    "bars = plt.bar(summary_temp['season'], summary_temp['mean'], yerr=summary_temp['std_dev'], capsize=5, color=['skyblue', 'lightcoral'])\n",
    "plt.ylabel('Average Deviation')\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_visible(False)\n",
    "\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLS for Project 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_vars = ['chair']\n",
    "df_dummies = pd.get_dummies(df_final, columns=categorical_vars, drop_first=True)\n",
    "\n",
    "#Drop one category for each dummy\n",
    "drop_columns = ['month','summer_spring','west_asia_dev','west_dev','asia','west','month_12','asia_dev','date','location','city', 'id1','id','temp0t7','gender','nat_name','c_asy_type','temp_daily', 'press6t4', 'press0t7', 'press_daily', 'dew6t4', 'dew0t7', 'dew_daily', 'prcp6t4', 'prcp0t7', 'prcp_daily', 'wind6t4', 'wind0t7', 'wind_daily', 'rh6t4', 'deviation', 'ltemp6t4', 'l2temp6t4', 'l3temp6t4', 'letemp6t4', 'le2temp6t4', 'le3temp6t4', 'beforetemp', 'aftertemp', 'temp22t8', 'press22t8', 'prcp22t8', 'wind22t8', 'dew22t8', 'temp4t4', 'press4t4', 'dew4t4', 'prcp4t4', 'wind4t4', 'lprcp', 'heat', 'comp_date', 'hearing_loc_code']\n",
    "df_dummies_clean = df_dummies.drop(drop_columns, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_v2 = df_dummies_clean['res']\n",
    "X_base = df_dummies_clean.drop(columns=['res'])\n",
    "X_v2 = sm.add_constant(X_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_v2 = Probit(Y, X.astype(float))  \n",
    "probit_model_v2 = model_v2.fit()\n",
    "print(probit_model_v2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stargazer_v2 = Stargazer([probit_model_v2])\n",
    "print(stargazer_v2.render_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Ridge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code in this section based on the notebook: ISLP-Ch06_varselect_lab.ipynb\n",
    "\n",
    "coefficients = []\n",
    "lambdas = 10**np.linspace(8, -2, 100) / Y_v2.std()\n",
    "scaler = StandardScaler(with_mean=True,  with_std=True)\n",
    "for lam in lambdas:\n",
    "    ridge = SGDClassifier(loss='log_loss', penalty='l2',alpha=lam)\n",
    "    pipe = Pipeline(steps=[('scaler', scaler), ('ridge', ridge)])\n",
    "    pipe.fit(X_v2, Y_v2)\n",
    "    coefficients.append(pipe.named_steps['ridge'].coef_.flatten())\n",
    "\n",
    "soln_array = np.array(coefficients)\n",
    "\n",
    "soln_path = pd.DataFrame(soln_array, columns=X_v2.columns, index=-np.log(lambdas))\n",
    "soln_path.index.name = 'negative log(lambda)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_fig, ax = subplots(figsize=(8,8))\n",
    "soln_path.plot(ax=ax, legend=False)\n",
    "ax.set_xlabel('$-\\log(\\lambda)$', fontsize=20)\n",
    "ax.set_ylabel('Standardized coefficients', fontsize=20)\n",
    "ax.legend(loc='upper left');\n",
    "\n",
    "path_fig.patch.set_facecolor('white')  \n",
    "ax.set_facecolor('white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=5)\n",
    "\n",
    "param_grid = {'ridge__alpha': lambdas}\n",
    "grid_search = GridSearchCV(pipe, param_grid, cv=kfold, scoring='accuracy', return_train_score=True)\n",
    "grid_search.fit(X_v2, Y_v2)\n",
    "tuned_ridge = grid_search.best_estimator_.named_steps['ridge']\n",
    "\n",
    "mean_scores = grid_search.cv_results_['mean_test_score']\n",
    "std_scores = grid_search.cv_results_['std_test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridgeCV_fig, ax = subplots(figsize=(8, 8))\n",
    "ax.errorbar(-np.log(lambdas), mean_scores, yerr=std_scores / np.sqrt(kfold.get_n_splits()), fmt='o')\n",
    "ax.axvline(-np.log(grid_search.best_params_['ridge__alpha']), c='k', ls='--')\n",
    "ax.set_xlabel('$-\\log(\\\\lambda)$', fontsize=20)\n",
    "ax.set_ylabel('Cross-validated Accuracy', fontsize=20)\n",
    "ax.set_title('Cross-Validation Accuracy with Error Bars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = tuned_ridge.coef_.flatten()  \n",
    "variable_names = X_v2.columns  \n",
    "\n",
    "coef_mapping = {variable: coef for variable, coef in zip(variable_names, coefficients)}\n",
    "coef_df = pd.DataFrame(list(coef_mapping.items()), columns=['Variable', 'Coefficient'])\n",
    "print(coef_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 306)  \n",
    "print(coef_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code in this section based on the notebook: ISLP-Ch06_varselect_lab.ipynb\n",
    "\n",
    "coefficients_l = []\n",
    "lambdas = 10**np.linspace(8, -2, 100) / Y.std()\n",
    "scaler = StandardScaler(with_mean=True,  with_std=True)\n",
    "for lam in lambdas:\n",
    "    lasso = SGDClassifier(loss='log_loss', penalty='l1',alpha=lam)\n",
    "    pipe_l = Pipeline(steps=[('scaler', scaler), ('lasso', lasso)])\n",
    "    pipe_l.fit(X_v2, Y_v2)\n",
    "    coefficients_l.append(pipe_l.named_steps['lasso'].coef_.flatten())\n",
    "\n",
    "soln_array_l = np.array(coefficients_l)\n",
    "soln_path_l = pd.DataFrame(soln_array_l, columns=X.columns, index=-np.log(lambdas))\n",
    "soln_path_l.index.name = 'negative log(lambda)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_fig_l, ax = subplots(figsize=(8,8))\n",
    "soln_path_l.plot(ax=ax, legend=False)\n",
    "ax.set_xlabel('$-\\log(\\lambda)$', fontsize=20)\n",
    "ax.set_ylabel('Standardized coefficients', fontsize=20)\n",
    "ax.legend(loc='upper left');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=5)\n",
    "\n",
    "param_grid_l = {'lasso__alpha': lambdas}\n",
    "\n",
    "\n",
    "grid_search_l = GridSearchCV(pipe_l, param_grid_l, cv=kfold, scoring='accuracy', return_train_score=True)\n",
    "grid_search_l.fit(X, Y)\n",
    "tuned_lasso = grid_search_l.best_estimator_.named_steps['lasso']\n",
    "\n",
    "mean_scores_l = grid_search_l.cv_results_['mean_test_score']\n",
    "std_scores_l = grid_search_l.cv_results_['std_test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lassoCV_fig, ax = subplots(figsize=(8, 8))\n",
    "ax.errorbar(-np.log(lambdas), mean_scores_l, yerr=std_scores_l / np.sqrt(kfold.get_n_splits()), fmt='o')\n",
    "ax.axvline(-np.log(grid_search_l.best_params_['lasso__alpha']), c='k', ls='--')\n",
    "ax.set_xlabel('$-\\log(\\\\lambda)$', fontsize=20)\n",
    "ax.set_ylabel('Cross-validated Accuracy', fontsize=20)\n",
    "ax.set_title('Cross-Validation Accuracy with Error Bars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients_l = tuned_lasso.coef_.flatten()  \n",
    "variable_names = X_v2.columns  \n",
    "\n",
    "coef_mapping_l = {variable: coef for variable, coef in zip(variable_names, coefficients_l)}\n",
    "\n",
    "coef_df = pd.DataFrame(list(coef_mapping_l.items()), columns=['Variable', 'Coefficient'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 306)  \n",
    "sorted_df = coef_df.sort_values(by='Coefficient', ascending=False)\n",
    "print(sorted_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresion Trees "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code in this section based on the notebook: ISLP-TreeModels.ipynb\n",
    "\n",
    "clf = DTC(criterion='entropy', \n",
    "          max_depth = 3,\n",
    "          random_state=0)   \n",
    "\n",
    "clf.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns\n",
    "#Rename columns for better labelling of trees\n",
    "X_detailed = ['Average temperature', 'Heat index', 'Sky coverage', 'Carbon monoxide levels', \n",
    "              'Distance CO source', 'Ozone levels', 'Distance ozone source', 'PM levels', \n",
    "              'Distance PM source', 'Atmospheric pressure', 'Dew point temperature', 'Precipitation', \n",
    "              'Wind speed', 'Relative humidity', 'Judge identifier', 'Asylum application', \n",
    "              'Gender', 'Middle Eastern', 'American', 'African', 'European', 'Northeast', 'Midwest', 'South', \n",
    "              '2000','2001', '2002', '2003', 'Interaction of temperature and Middle Eastern',\n",
    "              'Interaction of temperature and American', 'Interaction of temperature and African', \n",
    "              'Interaction of temperature and European', 'January', 'February', 'March', 'April', 'May', 'June', \n",
    "              'July', 'August', 'September', 'October', 'November']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = X_detailed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(Y, clf.predict(X)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "\n",
    "fig.patch.set_facecolor('white')  \n",
    "ax.set_facecolor('white')        \n",
    "\n",
    "plot_tree(clf,\n",
    "          feature_names=feature_names,\n",
    "          ax=ax,\n",
    "          filled=True,  \n",
    "          rounded=True,  \n",
    "          fontsize=7,   \n",
    "          proportion=True)  \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross validation \n",
    "validation = skm.ShuffleSplit(n_splits=1,\n",
    "                              test_size=200,\n",
    "                              random_state=0)\n",
    "results = skm.cross_validate(clf,\n",
    "                             X,\n",
    "                             Y,\n",
    "                             cv=validation)\n",
    "results['test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split dataset\n",
    "(X_train,\n",
    " X_test,\n",
    " Y_train,\n",
    " Y_test) = skm.train_test_split(X,\n",
    "                                   Y,\n",
    "                                   test_size=0.5,\n",
    "                                   random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DTC(criterion='entropy', random_state=0)\n",
    "clf.fit(X_train, Y_train)\n",
    "accuracy_score(Y_test, clf.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccp_path = clf.cost_complexity_pruning_path(X_train, Y_train)\n",
    "kfold = skm.KFold(5,\n",
    "                  random_state=1,\n",
    "                  shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code in this section based on the notebook: ISLP-TreeModels.ipynb\n",
    "\n",
    "bag_temperature = RFC(max_features=X_train.shape[1], random_state=0)\n",
    "bag_temperature.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_temperature = RFC(max_features=X_train.shape[1],\n",
    "                n_estimators=500,\n",
    "                random_state=0).fit(X_train, Y_train)\n",
    "y_hat_bag = bag_temperature.predict(X_test)\n",
    "accuracy_bagging = accuracy_score(Y_test, y_hat_bag)\n",
    "accuracy_bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_imp_bag = pd.DataFrame(\n",
    "    {'importance':bag_temperature.feature_importances_},\n",
    "    index=feature_names)\n",
    "feature_imp_bag.sort_values(by='importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_imp_bag = pd.DataFrame(\n",
    "    {'importance': bag_temperature.feature_importances_},\n",
    "    index=feature_names\n",
    ")\n",
    "\n",
    "# Sort the feature importances\n",
    "feature_imp_bag = feature_imp_bag.sort_values(by='importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_imp_bag.index, feature_imp_bag['importance'], color='skyblue')\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Feature Importances (Bagged Model)')\n",
    "plt.gca().invert_yaxis()  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code in this section based on the notebook: ISLP-TreeModels.ipynb\n",
    "\n",
    "RF_temperature = RFC(max_features=6,\n",
    "               random_state=0).fit(X_train, Y_train)\n",
    "y_hat_RF = RF_temperature.predict(X_test)\n",
    "accuracy_RF = accuracy_score(Y_test, y_hat_RF)\n",
    "accuracy_RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_imp = pd.DataFrame(\n",
    "    {'importance':RF_temperature.feature_importances_},\n",
    "    index=feature_names)\n",
    "feature_imp.sort_values(by='importance', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code in this section based on the notebook: ISLP-TreeModels.ipynb\n",
    "\n",
    "boost_temperature = GBC(n_estimators=500,\n",
    "                   learning_rate=0.001,\n",
    "                    max_depth = 3,\n",
    "                   random_state=0)\n",
    "boost_temperature.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boost_temperature = GBC(n_estimators=500,\n",
    "                   learning_rate=0.001,\n",
    "                    max_depth = 3,\n",
    "                   random_state=0)\n",
    "boost_temperature.fit(X_train,Y_train)\n",
    "y_hat_boost = boost_temperature.predict(X_test);\n",
    "accuracy_boosting = accuracy_score(Y_test, y_hat_boost)\n",
    "accuracy_boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_imp = pd.DataFrame(\n",
    "    {'importance':boost_temperature.feature_importances_},\n",
    "    index=feature_names)\n",
    "feature_imp.sort_values(by='importance', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directed Acyclic Graph (DAG) and Causal Relationship "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code in this section based on the notebook: CIDP-Chapter_04\n",
    "\n",
    "sample_gml = \"\"\"graph [\n",
    "directed 1\n",
    "\n",
    "node [\n",
    "    id 1\n",
    "    label \"Cognitive\"\n",
    "    ]\n",
    "    \n",
    "node [\n",
    "    id 2\n",
    "    label \"Weather\"\n",
    "    ]\n",
    "\n",
    "node [\n",
    "    id 4\n",
    "    label \"Resolution\"\n",
    "    ]\n",
    "    \n",
    "node [\n",
    "    id 5\n",
    "    label \"Location\"\n",
    "    ]\n",
    "\n",
    "node [\n",
    "    id 6\n",
    "    label \"Judge\"\n",
    "    ]\n",
    "\n",
    "node [\n",
    "    id 7\n",
    "    label \"Nationality\"\n",
    "    ]\n",
    "    \n",
    "edge [\n",
    "    source 2\n",
    "    target 1\n",
    "    ]\n",
    "\n",
    "edge [\n",
    "    source 5\n",
    "    target 4\n",
    "    ]\n",
    "\n",
    "edge [\n",
    "    source 6\n",
    "    target 4\n",
    "    ]\n",
    "\n",
    "edge [\n",
    "    source 1\n",
    "    target 6\n",
    "    ]\n",
    "edge [\n",
    "    source 7\n",
    "    target 4\n",
    "    ]\n",
    "edge [\n",
    "    source 5\n",
    "    target 2\n",
    "    ]\n",
    "]\n",
    "    \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = nx.parse_gml(sample_gml)\n",
    "\n",
    "nx.draw(\n",
    "    G=graph, \n",
    "    with_labels=True,\n",
    "    node_size=2500,\n",
    "    node_color=COLORS[0],\n",
    "    font_color='black',\n",
    "    font_size = 8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_gml2 = \"\"\"graph [\n",
    "directed 1\n",
    "\n",
    "node [\n",
    "    id 1\n",
    "    label \"Cognitive\"\n",
    "    ]\n",
    "    \n",
    "node [\n",
    "    id 2\n",
    "    label \"Weather\"\n",
    "    ]\n",
    "\n",
    "node [\n",
    "    id 4\n",
    "    label \"Resolution\"\n",
    "    ]\n",
    "    \n",
    "node [\n",
    "    id 5\n",
    "    label \"Location\"\n",
    "    ]\n",
    "\n",
    "node [\n",
    "    id 6\n",
    "    label \"Judge\"\n",
    "    ]\n",
    "\n",
    "node [\n",
    "    id 7\n",
    "    label \"Nationality\"\n",
    "    ]\n",
    "    \n",
    "edge [\n",
    "    source 2\n",
    "    target 1\n",
    "    ]\n",
    "\n",
    "edge [\n",
    "    source 5\n",
    "    target 4\n",
    "    ]\n",
    "\n",
    "edge [\n",
    "    source 6\n",
    "    target 4\n",
    "    ]\n",
    "\n",
    "edge [\n",
    "    source 1\n",
    "    target 6\n",
    "    ]\n",
    "edge [\n",
    "    source 7\n",
    "    target 1\n",
    "    ]\n",
    "edge [\n",
    "    source 5\n",
    "    target 2\n",
    "    ]\n",
    "]\n",
    "    \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = nx.parse_gml(sample_gml2)\n",
    "\n",
    "nx.draw(\n",
    "    G=graph, \n",
    "    with_labels=True,\n",
    "    node_size=2500,\n",
    "    node_color=COLORS[0],\n",
    "    font_color='black',\n",
    "    font_size = 8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_gml3 = \"\"\"graph [\n",
    "directed 1\n",
    "\n",
    "node [\n",
    "    id 1\n",
    "    label \"Cognitive\"\n",
    "    ]\n",
    "    \n",
    "node [\n",
    "    id 2\n",
    "    label \"Weather\"\n",
    "    ]\n",
    "\n",
    "node [\n",
    "    id 4\n",
    "    label \"Resolution\"\n",
    "    ]\n",
    "    \n",
    "node [\n",
    "    id 5\n",
    "    label \"Location\"\n",
    "    ]\n",
    "\n",
    "node [\n",
    "    id 6\n",
    "    label \"Judge\"\n",
    "    ]\n",
    "\n",
    "node [\n",
    "    id 7\n",
    "    label \"Nationality\"\n",
    "    ]\n",
    "    \n",
    "edge [\n",
    "    source 2\n",
    "    target 1\n",
    "    ]\n",
    "\n",
    "edge [\n",
    "    source 6\n",
    "    target 4\n",
    "    ]\n",
    "\n",
    "edge [\n",
    "    source 1\n",
    "    target 6\n",
    "    ]\n",
    "edge [\n",
    "    source 7\n",
    "    target 1\n",
    "    ]\n",
    "edge [\n",
    "    source 5\n",
    "    target 2\n",
    "    ]\n",
    "edge [\n",
    "    source 5\n",
    "    target 6\n",
    "    ]\n",
    "]\n",
    "    \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = nx.parse_gml(sample_gml3)\n",
    "\n",
    "nx.draw(\n",
    "    G=graph, \n",
    "    with_labels=True,\n",
    "    node_size=2500,\n",
    "    node_color=COLORS[0],\n",
    "    font_color='black',\n",
    "    font_size = 8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gml_final = \"\"\"graph [\n",
    "directed 1\n",
    "    \n",
    "node [\n",
    "    id 1\n",
    "    label \"midwest\"\n",
    "    ]\n",
    "\n",
    "node [\n",
    "    id 2\n",
    "    label \"deviation\"\n",
    "    ]\n",
    "\n",
    "node [\n",
    "    id 4\n",
    "    label \"res\"\n",
    "    ]\n",
    "    \n",
    "node [\n",
    "    id 5\n",
    "    label \"northeast\"\n",
    "    ]\n",
    "\n",
    "node [\n",
    "    id 6\n",
    "    label \"chair\"\n",
    "    ]\n",
    "\n",
    "node [\n",
    "    id 7\n",
    "    label \"america\"\n",
    "    ]\n",
    "node [\n",
    "    id 8\n",
    "    label \"south\"\n",
    "    ]\n",
    "node [\n",
    "    id 9\n",
    "    label \"west\"\n",
    "    ]\n",
    "node [\n",
    "    id 11\n",
    "    label \"cognitive\"\n",
    "    ]\n",
    "    \n",
    "edge [\n",
    "    source 2\n",
    "    target 11\n",
    "    ]\n",
    "\n",
    "edge [\n",
    "    source 6\n",
    "    target 4\n",
    "    ]\n",
    "\n",
    "edge [\n",
    "    source 7\n",
    "    target 11\n",
    "    ]\n",
    "edge [\n",
    "    source 5\n",
    "    target 2\n",
    "    ]\n",
    "edge [\n",
    "    source 1\n",
    "    target 2\n",
    "    ]\n",
    "edge [\n",
    "    source 8\n",
    "    target 2\n",
    "    ]\n",
    "edge [\n",
    "    source 9\n",
    "    target 2\n",
    "    ]\n",
    "edge [\n",
    "    source 5\n",
    "    target 11\n",
    "    ]\n",
    "edge [\n",
    "    source 1\n",
    "    target 11\n",
    "    ]\n",
    "edge [\n",
    "    source 8\n",
    "    target 11\n",
    "    ]\n",
    "edge [\n",
    "    source 9\n",
    "    target 11\n",
    "    ]\n",
    "edge [ \n",
    "    source 11\n",
    "    target 6\n",
    "    ]\n",
    "]\n",
    "\n",
    "    \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = nx.parse_gml(gml_final)\n",
    "\n",
    "nx.draw(\n",
    "    G=graph, \n",
    "    with_labels=True,\n",
    "    node_size=2500,\n",
    "    node_color=COLORS[0],\n",
    "    font_color='black',\n",
    "    font_size = 8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test DAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code in this section based on the notebook:  CIDP-Chapter_07\n",
    "\n",
    "model = CausalModel(\n",
    "data=df_final,\n",
    "treatment=['deviation'],\n",
    "outcome=\"res\",\n",
    "graph=gml_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimand = model.identify_effect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(estimand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimate = model.estimate_effect(\n",
    "identified_estimand=estimand,\n",
    "method_name=\"backdoor.linear_regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(estimate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Refutation test on whether estimate is influenced by unobserved confounders = random_common_cause \n",
    "refute_subset = model.refute_estimate(\n",
    "estimand=estimand,\n",
    "estimate=estimate,\n",
    "method_name=\"random_common_cause\",\n",
    "subset_fraction=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(refute_subset)\n",
    "#Note: High p-value suuggests that the random common cause does not have a meaningful impact on the relationship between \n",
    "#temperature and the outcome, providing confidence in the stability of findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IPW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code in this section based on the notebook: CIBT-11-Propensity-Score\n",
    "\n",
    "#Changed for deviation, being the treatment variable because if we had only temperature \n",
    "#we might have that specific regions such as Texas is always treated etc.\n",
    "df_final['T_binary'] = (df_final['deviation'] > 0.000095).astype(int)\n",
    "print(df_final['T_binary'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 'T_binary'\n",
    "Y = 'res'\n",
    "X = ['chair', 'dummy_asylum', 'dummy_gender', \n",
    "                     'middleast', 'america', 'africa', 'europe', 'northeast', 'midwest', \n",
    "                     'south', 'year2000', 'year2001', 'year2002', \n",
    "                     'year2003','month_1',\n",
    "                     'month_2','month_3','month_4','month_5','month_6','month_7','month_8',\n",
    "                     'month_9','month_10','month_11']\n",
    "\n",
    "ps_model = LogisticRegression(C=1e6).fit(df_final[X], df_final[T])\n",
    "\n",
    "data_ps = df_final.assign(propensity_score=ps_model.predict_proba(df_final[X])[:, 1])\n",
    "\n",
    "data_ps[[\"T_binary\", \"res\", \"propensity_score\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_t = 1/data_ps.query(\"T_binary==1\")[\"propensity_score\"]\n",
    "weight_nt = 1/(1-data_ps.query(\"T_binary==0\")[\"propensity_score\"])\n",
    "print(\"Original Sample Size\", df.shape[0])\n",
    "print(\"Treated Population Sample Size\", sum(weight_t))\n",
    "print(\"Untreated Population Sample Size\", sum(weight_nt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(data_ps.query(\"T_binary==0\")[\"propensity_score\"], kde=False, label=\"Non Treated\")\n",
    "sns.distplot(data_ps.query(\"T_binary==1\")[\"propensity_score\"], kde=False, label=\"Treated\")\n",
    "plt.title(\"Positivity Check\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove observations with propensity score = 1\n",
    "data_ps = data_ps[data_ps[\"propensity_score\"] < 1]\n",
    "\n",
    "treated_data = data_ps.query(\"T_binary == 1\")\n",
    "control_data = data_ps.query(\"T_binary == 0\")\n",
    "\n",
    "y1 = sum(treated_data[\"T_binary\"] * weight_t) / len(treated_data)\n",
    "y0 = sum(control_data[\"T_binary\"] * weight_nt) / len(control_data)\n",
    "\n",
    "ate = np.mean(weight_t * treated_data[\"T_binary\"]) - np.mean(weight_nt * control_data[\"T_binary\"])\n",
    "\n",
    "print(\"Y1:\", y1)\n",
    "print(\"Y0:\", y0)\n",
    "print(\"ATE:\", ate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate weights for treated and control groups\n",
    "treated_data = data_ps.query(\"res == 1\")\n",
    "control_data = data_ps.query(\"res == 0\")\n",
    "\n",
    "if not treated_data.empty:\n",
    "    weight_t = 1 / treated_data[\"propensity_score\"]\n",
    "    print(\"Weight_t:\", weight_t)\n",
    "\n",
    "if not control_data.empty:\n",
    "    weight_nt = 1 / (1 - control_data[\"propensity_score\"])\n",
    "    print(\"Weight_nt:\", weight_nt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_ps[\"propensity_score\"].min(), data_ps[\"propensity_score\"].max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ps(df_final, X, T, y):\n",
    "    ps = LogisticRegression(C=1e6, max_iter=2000, solver='liblinear').fit(df_final[X], df_final[T]).predict_proba(df_final[X])[:, 1]\n",
    "    weight = (df_final[T]-ps) / (ps*(1-ps)) \n",
    "    return np.mean(weight * df_final[y]) \n",
    "\n",
    "sample_df = df_final.sample(frac=1, replace=True)\n",
    "ate = run_ps(sample_df, X, T, Y)\n",
    "print(ate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = df_final.sample(frac=0.1, replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ps(sample_df, X, T, y):\n",
    "    ps = LogisticRegression(C=1e6, max_iter=2000, solver='liblinear').fit(sample_df[X], sample_df[T]).predict_proba(sample_df[X])[:, 1]\n",
    "    weight = (sample_df[T]-ps) / (ps*(1-ps)) \n",
    "    return np.mean(weight * sample_df[y]) \n",
    "\n",
    "np.random.seed(88)\n",
    "bootstrap_sample = 100\n",
    "ates = Parallel(n_jobs=4)(delayed(run_ps)(sample_df.sample(frac=1, replace=True), X, T, Y)\n",
    "                          for _ in range(bootstrap_sample))\n",
    "ates = np.array(ates)\n",
    "ates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(ates, kde=False)\n",
    "plt.vlines(np.percentile(ates, 2.5), 0, 30, linestyles=\"dotted\")\n",
    "plt.vlines(np.percentile(ates, 97.5), 0, 30, linestyles=\"dotted\", label=\"95% CI\")\n",
    "plt.legend();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
